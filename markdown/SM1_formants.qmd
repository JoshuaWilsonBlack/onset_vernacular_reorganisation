---
title: "SM1: Formant tracking, preprocessing, and normalisation"
author:
    - name: Joshua Wilson Black
      orcid: 0000-0002-8272-5763
      email: joshua.black@canterbury.ac.nz
      affiliations:
      - name: New Zealand Institute for Language, Brain and Behaviour, University of Canterbury
        city: Christchurch
        country: New Zealand
    - name: Lynn Clark
      orcid: 0000-0003-3282-6555
      email: lynn.clark@canterbury.ac.nz
      affiliations:
        - name: Department of Linguistics, University of Canterbury
          city: Christchurch
          country: New Zealand
        - name: New Zealand Institute for Language, Brain and Behaviour, University of Canterbury
          city: Christchurch
          country: New Zealand
date: today
lightbox:
  match: auto
format: 
  html:
    theme: flatly
    toc: true
    toc-depth: 6
    toc-expand: true
    toc-location: right
    code-summary: "To view code click here"
    anchor-sections: true
    number-sections: true
    cap-location: margin
    title-block-banner: true
    fig-responsive: true
    lang: 'en-GB'
    execute:
      warning: false
    embed-resources: false
bibliography: 
  - "https://api.citedrive.com/bib/1e7eade6-e5e0-4cea-96b3-27a813f7bd4a/references.bib?x=eyJpZCI6ICIxZTdlYWRlNi1lNWUwLTRjZWEtOTZiMy0yN2E4MTNmN2JkNGEiLCAidXNlciI6ICI0NzAyIiwgInNpZ25hdHVyZSI6ICJkOWIxODViNzJiOWNmNDQ1ZWVjMGU4NzNhMzM4ODg2NWUyYTQ3MzgzMDQ3ZjIxZWIwZDUxNTBkYThkMzE1M2VkIn0=/bibliography.bib"
  - ../grateful-refs.bib
editor: 
  markdown: 
    wrap: 72
---

# Overview

This document sets out the R code used to extract all monophthongs,
track formants using the FastTrack plugin for Praat, filter, and normalise.

Formant tracking was carried out using `FastTrack` in `Praat`. Instructions to
reproduce our formant extraction method are provided below and a validation of
the `FastTrack` settings we used is provided in @sec-validation.

Note that we extract all monophthongs from the corpus in this document, while we
only analyse the 'extended short front vowel shift' in the associated paper. 
The full complement of monophthongs is necessary for normalisation.

# Libraries and `renv` environment

We load a series of libraries.

```{r}
# Tidyverse and related packages
library(tidyverse)
library(glue)
library(lubridate)
library(ggrepel)

# improved tables
library(kableExtra)

# File path management
library(here)

# nzilbb.labbcat allows access to our LaBB-CAT instance.
library(nzilbb.labbcat)

# nzilbb.vowels provides a series of functions to help with processing vowel
# data.
library(nzilbb.vowels)

# To construct a diagram of the filtering process.
library(DiagrammeR)

# To plot correlations
library(ggcorrplot)

# To load images from file
library(imager)

# We use fasttrackr as part of our workflow for using FastTrack with Praat.
# (Rather than directly using fasttrackr to replace the Praat integration).
library(fasttrackr)

# We use rPraat is used to load midpoint readings.
library(rPraat)

labbcat.url <- 'https://labbcat.canterbury.ac.nz/kids/'

# Set ggplot theme
theme_set(theme_bw())

# Set colours for vowel plots
# These are derived from Tol's rainbow palette, designed
# to be maximally readable for various forms of colour 
# blindness (https://personal.sron.nl/~pault/).
vowel_colours <- c(
  DRESS = "#777777",
  FLEECE = "#882E72",
  GOOSE = "#4EB265",
  KIT = "#7BAFDE",
  LOT = "#DC050C",
  TRAP = "#F7F056",
  START = "#1965B0",
  STRUT = "#F4A736",
  THOUGHT = "#72190E",
  NURSE = "#E8601C",
  FOOT = "#5289C7"
)

# It is sometimes useful for filtering to have a list of all monophthongs.
monophthongs <- c(
  "DRESS", "FLEECE", "GOOSE", "KIT", "LOT", "TRAP", "START", "STRUT",
  "THOUGHT", "NURSE", "FOOT", "SCHWA"
)

# Set a seed for the random sampling below
set.seed(10)
```

We use `renv` to store and restore the specific versions of packages we
use. For more information see
<https://rstudio.github.io/renv/articles/renv.html>. Install `renv` if you 
do not have the package.

To install `nzilbb.vowels`, it is best to restore the `renv` environment
included in the GitHub repository (using `renv::restore()`). If you don't want
to use `renv`, then use `remotes::install_github('nzilbb/nzilbb_vowels')` (you
may need to install the `remotes` package. This installs from the GitHub
repository rather than CRAN.^[
The `nzilbb.vowels` package will be submitted to CRAN in the near future. 
Any future version on CRAN should be compatible with this code.
] The same instructions apply to `fasttrackr`, which
can be installed from `renv` or directly from `github`. Simply replace
`'nzilbb/nzilbb_vowels'` with `'santiagobarreda/FastTrackR'` in the call to
`install_github()`.

# LaBB-CAT extraction

Transcripts and audio of story retell tasks were uploaded to a LaBB-CAT instance
[@fromont-hay-2012-labb]. Details of the forced alignment are given in the paper.

We extract all monophthong tokens from our LaBB-CAT instance using the
`nzilbb.labbcat` package. The code in the block below will only run successfully
if you have access to the LaBB-CAT instance. It is provided
for documentation purposes only.

It is worth noting here that there is a two-stage process where the `segment`
layer is generated by MFA using an American English phonemic dictionary. From
this, we generate a `celexSegment` layer which contains the most likely 
pronunciation form the CELEX dictionary.

```{r}
#| eval: false
#| code-fold: true

# Define the search pattern
pattern <- list(
  columns = list(
    list(
      layers = list(
        # We want all vowels in the segment layer. The pattern is a
        # regular expression, using the DISC alphabet.
        # We will later filter to include only the monophthongs from the 
        # celexSegment layer.
        segment = list(pattern = "[I@EVQU{i#3\\$u123456789]")
      )
    )  
  )
)

# Collect matching tokens.
# anchor.confidence.min gives us all automatically aligned tokens.
# We will filter these to include only those transcriptions marked as manually
# checked.
matches <- getMatches(
  labbcat.url,
  pattern = pattern,
  page.length = 1000,
  anchor.confidence.min = 50,
  # only extract transcripts which have been manually corrected.
  transcript.expression = expressionFromAttributeValue(
    'transcript_phonemic_corrections', 1
  )
)
# Tokens: 19446

# Collect relevant information at the transcript level. This includes the 
# age of the child when the transcript was recorded, where it was recorded, 
# etc.
transcript_attributes <- getTranscriptAttributes(
  labbcat.url,
  unique(matches$Transcript),
  c(
    'transcript_cp', 'transcript_age', 'transcript_centre',
    'transcript_recording_date'
  )
)

# Merge transcript-level information with matches by left join.
matches <- matches |>
  rename(
    transcript = Transcript,
    participant = Participant
  ) |>
  left_join(transcript_attributes)

# Collect relevant information at participant level.
participant_attributes <- getParticipantAttributes(
  labbcat.url,
  unique(matches$participant),
  c(
    'participant_date_of_birth', 'participant_gender', 
    'participant_ethnicity', 'participant_home_language'
  )
)

# Merge participant-level information with matches by left join.
matches <- matches |> 
    left_join(participant_attributes)

# Collect match labels (additional information at the token level)
match_labels <- getMatchLabels(
  labbcat.url,
  match.ids = matches$MatchId,
  layer.ids = c(
    "celexSegment", "noise",
    "syllable", "orthography",
    "celex_frequency", 'corpus_frequency'
  )
)

# Collect the segment following each token.
following_seg <- getMatchLabels(
  labbcat.url, 
  matches$MatchId, 
  layer.ids = c("segment"),
  target.offset=1
)

matches <- bind_cols(matches, match_labels, following_seg)

# Use Praat integration to collect pitch information
pitches <- processWithPraat(
  labbcat.url,
  matches$MatchId, matches$Target.segment.start, matches$Target.segment.end,
  window.offset = 0.025,
  praatScriptPitch(
    get.mean = TRUE,
    get.minimum = TRUE,
    get.maximum = TRUE,
    time.step = 0,
    pitch.floor = 150,
    max.number.of.candidates = 15,
    very.accurate = FALSE,
    silence.threshold = 0.03,
    voicing.threshold = 0.5,
    octave.cost = 0.01,
    octave.jump.cost = 0.35,
    voiced.unvoiced.cost = 0.35,
    pitch.ceiling = 1250, # On the high side, some children can reach almost 2000 Hz when yelling
    # https://www.fon.hum.uva.nl/praat/manual/Intro_4_2__Configuring_the_pitch_contour.html
    pitch.floor.male = NULL,
    voicing.threshold.male = 0.4,
    pitch.ceiling.male = NULL,
    gender.attribute = "participant_gender",
    value.for.male = "M",
    sample.points = NULL,
    interpolation = "linear",
    skip.errors = TRUE
  )
)

matches <- bind_cols(matches, pitches) |> select(-Error)

# Tidy names, generate useful columns, remove unnecessary columns.
matches <- matches |> 
  rename(
    start = Target.segment.start,
    end = Target.segment.end,
    word_start = Target.word.start,
    word_end = Target.word.end,
    age = transcript_age,
    dob = participant_date_of_birth,
    gender = participant_gender,
    collect = transcript_cp,
    ethnicity = participant_ethnicity,
    home_language = participant_home_language,
    text = Text,
    word = orthography,
    following_segment = Token.plus.1.segment
  ) |> 
  mutate(
    # the mfa_vowel variable uses the segment layer and represents the best
    # guess of the mfa model given it's available pronunciations.
    mfa_vowel = fct_recode(
      factor(Target.segment),
      FLEECE = "i",
      KIT = "I",
      DRESS = "E",
      TRAP = "{",
      START = "#",
      LOT = "Q",
      THOUGHT = "$",
      NURSE = "3",
      STRUT = "V",
      FOOT = "U",
      GOOSE = "u",
      SCHWA = "@"
    ),
    # Remove consonants added when converting from mfa segments to celex
    # phobemes. This code uses the fact that '[^X]' matches all characters
    # _except_ x.
    vowel = str_replace(
      celexSegment,
      "[^iIE{#\\$3QVUu@123456789]",
      ""
    ),
    # The vowel variable will have some DISC coded diphthongs in it. e.g.
    # '8' (= SQUARE)
    vowel = fct_recode(
      factor(vowel),
      FLEECE = "i",
      KIT = "I",
      DRESS = "E",
      TRAP = "{",
      START = "#",
      LOT = "Q",
      THOUGHT = "$",
      NURSE = "3",
      STRUT = "V",
      FOOT = "U",
      GOOSE = "u",
      SCHWA = "@"
    ),
    stress = case_when(
      str_detect(syllable, '".*') ~ '"',
      str_detect(syllable, "'.*") ~ "'",
      .default = "0"
    )
  ) |> 
  select(
    -Target.segment, -Corpus, -Line, -LineEnd,
    -Before.Match, -After.Match, -Number, -Target.word, -SearchName
  )

# Filter non-monophthongs (in celex_vowel layer)
matches <- matches |> 
  filter(
    vowel %in% monophthongs
  ) |> 
  mutate(
    vowel = factor(vowel, levels = monophthongs)
  )
# tokens: 16045

# Correct ethnicity error in participant.
matches <- matches |> 
  mutate(
    ethnicity = if_else(ethnicity == "MÄori", "Māori", ethnicity)
  )

# Save data
write_rds(matches, here('data', 'untracked_vowels.rds'))
write_csv(matches, here('data', 'untracked_vowels.csv'))
```

We now load the data extracted from LaBB-CAT.

```{r}
vowels <- read_rds(here('data', 'untracked_vowels.rds'))
```

We use word frequency information derived from the [CHILDES](https://childes.talkbank.org/) corpus. The script
which extracts this information is given in the GitHub repository for the
project at `scripts/get_childes_freq.R`. The script also contains a list of the 
corpora we use and links to their descriptions on the CHILDES website.

```{r}
childes_counts <- read_rds(here('data', 'CHILDES_tokens.rds'))

# Rename to enable join and to explicitly distinguish childes information form
# the corpus frequency information.
childes_counts <- childes_counts |>
      rename(
        word = gloss,
        childes_count = count,
        childes_rate = perc
      ) |>
      select(
        word, childes_count, childes_rate
      )

vowels <- vowels |>
  left_join(childes_counts)
```

There are `r nrow(vowels)` tokens. The column names are:

```{r}
names(vowels)
```

The important columns to note are:

-   `participant` contains a unique participant identified.
-   `MatchId` and `URL` connect the each segment with the LaBB-CAT
    corpus.
-   `mfa_vowel` indicates the vowel detected by MFA which the segment comes 
from stored as a Wells lexical set.
-   `vowel` indicates the best CELEX phoneme to match the segments found by MFA.
-   `start` and `end` give the onset and offset of the segment (seconds).
-   `minPitch`, `maxPitch` and `meanPitch` give pitch information for
    the segment extracted via Praat.
-   `following_segment` gives the CELEX DISC code for the following
    segment.
-   `syllable` and `stress` tell us what syllable the segment comes from
    and whether it is stressed.
-   `text` and `word` give the word in which the segment appears with
    punctuation and without, respectively.
-   `corpus_frequency` and `celex_frequency` give frequency from the
    corpus and from CELEX, respectively.
-   `childes_count` gives frequency of the word in a subset of the CHILDES 
    corpus, while `childes_rate` gives the rate.
-   `word_start` and `word_end` give the onset and offset of the word
    which contains the segment (seconds).
-   `dob`, `gender`, `ethnicity`, and `home_language` provide metadata
    about the child.
-   `collect` indicates the collection point at which the segment was
    recorded (1, 2, 3, or 4).
-   `transcript_centre` and `transcript_recording_date` tells us where
    and when the transcript was recorded.

We will give more detailed summary information after the filtering and 
preprocessing steps (@sec-data-desc).

# Formant tracking with FastTrack

FastTrack [@barreda2021fast] provides a more robust alternative to the
default automatic formant tracking method in Praat [@boersma2024praat]. 
FastTrack is a Praat plugin which fits a
series of alternative formant tracks, fits polynomial models through
each alternative track, and decides on a winner on the basis of a series of
heuristics applied to the formant tracks and models.

FastTrack has a series of user-adjustable options. Barreda [-@barreda2021fast]
recommends that users adjust these options to match their speakers.

Our first approach to formant tracking applied FastTrack using LaBB-CAT's
integration with FastTrack. However, our need to track the formants of
very young child speech required us to adjust settings which are unavailable 
through LaBB-CAT. In particular, we applied hard boundaries for the possible
frequency values of formants from some vowel types. Additionally, we needed to
examine alternative analyses produced by FastTrack to determine how and why 
our initial attempts were failing. Consequently, we decided to process the
formants by downloading the audio files from LaBB-CAT and applying FastTrack
locally.

The path we took may very well be useful to other researchers, so we present
it in some detail here. We first present our original formant tracking and
show what went wrong (@sec-labbcat-int). We then set out the method for
extracting the required audio, locally processing it, and determining 
appropriate FastTrack settings (@sec-local). Finally, we validate the settings
we decided on by spot checking 30 tokens from each vowel (@sec-validation).

## LaBB-CAT integration {#sec-labbcat-int}

LaBB-CAT integrates with FastTrack and allows control of many of the options
for formant extraction. Our initial formant extraction used 
FastTrack recommendations for speakers less than five foot tall (152cm).
This cut off height is significantly higher than the expected height of a five
year old in New Zealand. In New Zealand, the median height for five year old
boys is 110cm and for five year old girls is 109cm ([Te Whatu Ora](https://www.tewhatuora.govt.nz/for-the-health-sector/specific-life-stage-health-information/child-health/well-child-tamariki-programme/growth-charts)).

The code below again requires access to our LaBB-CAT instance, and is provided
for documentation purposes only. Behind the scenes, this document loads the
result of running this code so that subsequent blocks will run.

```{r}
#| eval: false
#| code-fold: true
formants <- processWithPraat(
  labbcat.url,
  vowels$MatchId, vowels$start, vowels$end,
  praatScriptFastTrack(
    formants = c(1,2),
    sample.points = c(0.5),
    lowest.analysis.frequency = 5500,
    # Use of NULL here turns off differentiation for male and female speakers.
    lowest.analysis.frequency.male = NULL,
    highest.analysis.frequency = 7500,
    highest.analysis.frequency.male = NULL,
    gender.attribute = "participant_gender",
    value.for.male = "M",
    time.step = 0.002,
    tracking.method = "burg",
    number.of.formants = 3,
    maximum.f1.frequency = 1200,
    maximum.f1.bandwidth = NULL,
    maximum.f2.bandwidth = NULL,
    maximum.f3.bandwidth = NULL,
    minimum.f4.frequency = 2900,
    enable.rhotic.heuristic = TRUE,
    enable.f3.f4.proximity.heuristic = TRUE,
    number.of.steps = 24,
    number.of.coefficients = 5
  ),
  # Take audio 0.025s either side of token start and end.
  window.offset = 0.025
)

vowels <- bind_cols(vowels, formants)

vowels <- vowels |> 
  rename(
    time = time_0_5,
    F1 = f1_time_0_5,
    F2 = f2_time_0_5
  )

write_rds(vowels, here('data', 'vowels_default_ft.rds'))
```

```{r}
#| include: false
vowels <- read_rds(here('data', 'vowels_default_ft.rds'))
```

We'll now visualise the default formant readings in the vowel space. 

First, we define a plotting function.

```{r}
plot_vs <- function(in_data, vowel_colours, alpha_level = 0.2) {
  
  means <- in_data |> 
    group_by(
      vowel
    ) |> 
    summarise(
      F1 = mean(F1, na.rm=TRUE),
      F2 = mean(F2, na.rm=TRUE),
      winner = "MEAN"
    )
  
  out_plot <- in_data |> 
    ggplot(
      aes(
        x = F2, 
        y = F1, 
        colour = vowel
      )
    ) + 
    geom_point(alpha = alpha_level, size = 1) + 
    stat_ellipse(level=0.67, linewidth=1) +
    geom_point(data = means, size = 4) +
    scale_x_reverse(limits = c(4500, 0)) + 
    scale_y_reverse(limits = c(2000, 0)) + 
    scale_colour_manual(values=vowel_colours)
  
  out_plot
}
```

We then apply the function: 

```{r}
#| label: fig-all-default
#| fig.cap: "All monophthong tokens, with default FastTrack settings for speakers below 5ft."
ft_default_plot <- vowels |> 
  plot_vs(vowel_colours=vowel_colours) +
  labs(
    title = "F1 and F2 for All Monophthongs",
    subtitle = "FastTrack Defaults, 1 s.d. ellipses.",
    colour = "Vowel"
  )

ft_default_plot
```

@fig-all-default shows a large number of high-F1 outliers. This is clear at the
bottom of the vowel space plot. However, the biggest problem is the _width_
of the ellipses for our high front vowels.

This is clearer if we look at a subset of the vowels. In this case,
<span style="font-variant: small-caps;">fleece</span>, 
<span style="font-variant: small-caps;">dress</span> and <span style="font-variant: small-caps;">trap</span>.

```{r}
#| label: fig-high-default
#| fig-cap: "All FLEECE, DRESS, and TRAP tokens, with default FastTrack settings for speakers below 5ft."
vowels |>
  filter(
    vowel %in% c('FLEECE', 'DRESS', 'TRAP')
  ) |>
  plot_vs(
    vowel_colours = vowel_colours,
    alpha_level = 0.3
  ) + 
  labs(
    title = "F1 and F2 for FLEECE, DRESS, and TRAP Monophthongs",
    subtitle = "FastTrack Defaults, 1 s.d. ellipses",
    colour = "Vowel"
  )
```

There is an extreme spread of F2 values for these vowels. The ellipses suggest a
majority of the tokens are in sensible places, but there are a _very_ large
number of implausible values in the data frame. In particular, none of these
tokens should be at the very back of the vowel space.

To determine the origin of these bad values, we looked at a sample of tokens and
determine if alternative FastTrack settings would be appropriate. As noted
above, this required us to process these tokens locally.

## Local processing {#sec-local}

### Determine FastTrack settings

There are two broad categories of setting which we can modify: 

1. The range of upper frequency limits within which FastTrack generates and
compares analyses.
2. Hard limits on the mean value of each formant.

We will look at a sample of analyses of vowel tokens to determine how we should
modify our settings. We'll start the process by making some changes to 
upper frequency limits on the 
basis of the plots we have already seen (@fig-all-default). 

The natural hypothesis is that we have set the maximum frequency too low to
capture small children's speech. To see a particularly clear example of what can
happen when the maximum frequency is set too low, look at the following 'before'
and 'after' plots (@fig-example).

::: {#fig-example layout-ncol=2}

![Before (Max frequency = 6457Hz)](../images/before_AV6E_ZCBCOI5R_T1_80.png){#fig-before}

![After (Max frequency = 8870Hz](../images/after_AV6E_ZCBCOI5R_T1_80.png){#fig-after}

Before and after changes to FastTrack settings. The green line indicates F2 and the formant cut off for the analysis is at the top of each plot.
:::

In this kind of case, the only place for the second formant to go using the
default settings is in the empty(ish) space between, roughly, 1000Hz and 3500Hz. 

@fig-all-default suggests that the front vowel have a bigger problem here than
the others (their F2 values are higher, so they are more likely to run in to 
this problem). 

So we set the following range of upper frequency cut off values for the 'front',
'back' or 'other' vowels:

- 'Front vowels' (<span style="font-variant: small-caps;">fleece, dress, nurse, goose, trap, kit</span>): 6000-9000Hz.
- 'Back vowels' (<span style="font-variant: small-caps;">lot, thought, foot</span>): 5000-8000Hz.
- 'Other vowels' (<span style="font-variant: small-caps;">strut, start, schwa</span>): 5500-8500Hz

For comparison, recall that the original settings were 5500 to 7500Hz for all
vowels.

We apply these settings to a random sample of vowel tokens and then determine
whether or not there is any evidence that the range needs to be changed and
whether the addition of hard limits on formant ranges would enable FastTrack to
select a better analysis.

The code block below (folded by default) generates the samples (n=25 for each
vowel) and extracts the relevant audio from LaBB-CAT. Again, it requires access
to the LaBB-CAT instance.

```{r}
#| code-fold: true
#| eval: false
vowel_types <- vowels$vowel |> unique()

# This dataframe will contain the samples from each vowel
samples <- tibble(
  vowel = vowel_types
)

# Create a function to apply to each vowel type to generate a sample of 25.
create_sample <- function(vowel_type) {
  # Create directory to hold the sample.
  dir.create(here('praat', vowel_type), recursive = TRUE)
  
  # Create sample.
  sample_vowels <- vowels |> 
    filter(
      vowel == vowel_type
    ) |> 
    slice_sample(n=25)
  
  # Get sound files from LaBB-CAT
  sample_files <- getSoundFragments(
    labbcat.url,
    ids = sample_vowels$transcript,
    start.offsets = sample_vowels$start - 0.025,
    end.offsets = sample_vowels$end - 0.025,
    path = here("praat", vowel_type, "sounds")
  )
  
  # Get filename.
  sample_vowels <- sample_vowels |> 
    mutate(
      file = str_extract(sample_files, "[A-Za-z0-9_\\.\\-]*$")
    )
  
  # Processing with FastTrack requires file information. This is
  # created here using the `fasttrackr` package's `makefileinformation`
  # function.
  sample_info <- makefileinformation(here('praat', vowel_type))

  sample_info <- sample_info |> 
    left_join(
      sample_vowels |> 
        select(file, vowel)
    ) |> 
    select(
      -label
    ) |> 
    rename(
      label = vowel
    ) |> 
    relocate(number, file, label)

  # Write the file information to the relevant folder.
  write_csv(sample_info, here('praat', vowel_type, 'file_information.csv'))
  
  # Return the sample.
  sample_vowels
}

# Apply function to all vowel types.
samples <- samples |> 
  mutate(
    sample = map(vowel, create_sample)
  )

# We save information about the samples.
write_rds(samples, here('data', 'samples.rds'))
```

```{r}
#| include: false
samples <- read_rds(here('data', 'samples.rds'))
```


In order to process these audio files, we used FastTrack with Praat
version 6.1.09. FastTrack does not have version numbers, but was downloaded
on the 9th of March, 2023. The version of FastTrack available on that date 
can be found [here](https://github.com/santiagobarreda/FastTrack/tree/3d0482f4675aed85bc15635a4af423440b3092fc). We used the 'Track folder...' option visible when Praat is opened.

We used 5 coefficient for prediction, 24 candidate analyses within the limits
set, a time step of 0.002 seconds, the Burg tracking method, the `dct` basis
function and `mae` errors. We enabled the F1 < 1200Hz, F4 > 2900Hz, and F3 and
F4 proximity heuristics and turned off the rhotic heuristic.

If you are repeating these steps, make sure to tick the 'Make images comparing
analyses' box. If you do not, you will be unable to determine whether hard
frequency limits might improve your results.

#### Vowel-by-vowel {#sec-vowel-by-vowel}

Working through these samples was a long process. The process, for each vowel,
is summarised in the tabs below.

The code folded below loads the formants selected by FastTrack.
```{r}
#| code-fold: true

# Load the winning values from FastTrack. We read in the `aggregated_data.csv`
# file produced by FastTrack for each vowel type and analysis and the
# `winners.csv` file to tell us which of the analyses tried by FastTrack was
# considered best.
fasttrack_info <- samples |> 
  mutate(
    ft_data = map(vowel, ~ read_csv(
        here('praat', .x, 'processed_data', 'aggregated_data.csv')
      )
    ),
    winners = map(vowel, ~ read_csv(here('praat', .x, 'winners.csv')) |> select(winner))
  ) |> 
  select(vowel, ft_data, winners) |> 
  unnest(c(ft_data, winners))

# Fasttrack will ignore files which it cannot fit models to. Usually this is 
# because the token is too short. We want to know about these, for the purposes
# of the test below. The following code adds these tokens to the data from 
# the samples.

fasttrack_vowels <- samples |>
  select(sample) |> 
  unnest(sample) |> 
  mutate(
    # remove '.wav' from filename. Enables left join below.
    file = str_sub(file, end = -5)
  ) |> 
  select(vowel, file, word, start, end, MatchId, URL, noise, meanPitch) |> 
  mutate(
    token_dur = end - start
  ) |> 
  left_join(fasttrack_info, by = c("vowel", "file")) |> 
  mutate(
    # if there is no 'winner' value, the token hasn't been tracked by 
    # fasttrack.
    tracked = !is.na(winner)
  )
```

In order to plot the results of our local processing, we slightly modify the
previous vowel space plotting function.

```{r}
plot_vs_ft <- function(in_data) {
  
  means <- in_data |> 
    group_by(
      vowel
    ) |> 
    summarise(
      f1 = mean(f1, na.rm=TRUE),
      f2 = mean(f2, na.rm=TRUE),
      winner = "MEAN"
    )
  
  out_plot <- in_data |> 
    ggplot(
      aes(
        x = f2, 
        y = f1, 
        colour = vowel, 
        label = winner
      )
    ) + 
    geom_text(alpha = 0.8, size = 2) + 
    stat_ellipse() +
    geom_point(data = means, size = 4) +
    scale_x_reverse(limits = c(4500, 0)) + 
    scale_y_reverse(limits = c(2000, 0)) + 
    scale_colour_manual(values=vowel_colours) +
    labs(x = "F2", y = "F1")
  
  out_plot
}
```

We then generate an overall plot:

```{r}
fasttrack_vowels <- fasttrack_vowels |> 
  rename(
    f1 = f13,
    f2 = f23
  )

fasttrack_vowels |> 
  plot_vs_ft()
```

We still have some very wide ellipses. This is not surprising given lower token 
count (25 for each vowel). We'll look at these in more detail, vowel-by-vowel,
below.

Which analysis tends to win for each vowel type? That is, which of the 24 steps between the minimum
frequency and the maximum analysis frequency is the winning analysis? 
We display this in a histogram:

```{r}
fasttrack_vowels |> 
  ggplot(
    aes(
      x = winner,
      fill = vowel
    )
  ) +
  geom_histogram(bins=24)
```

The 'winner' values in this plot are the 'step' which won. The steps move from
the lowest allowable maximum frequency to the highest. We see here that the 1st
and 24th steps win more than the other analyses. In such cases, it may be that
the true analysis would be found at a more extreme value.

This plot suggests, for instance, that there might be a problem with the 
[thought]{.smallcaps} vowel. The relative majority of
[thought]{.smallcaps} tokens seem to have the first analysis as the winner. This
suggests that perhaps the lower bound needs to be lower. We will examine this
further below.

For each vowel, we will plot the
winning analyses from the sample in F1/F2 space with a number indicating which
of the analyses 'won'. The panel below enables you to switch between vowels.
At the bottom of each panel, a decision will be made about the appropriate 
FastTrack settings for the relevant vowel.

::: {.panel-tabset}

##### DRESS

```{r}
#| label: fig-dress-sample
#| fig-cap: "DRESS tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "DRESS"
  ) |> 
  plot_vs_ft()
```

We will look at the three tokens with very low F2 and F1, indicated with '2', '1'
and '1' in the top right. We'll also look at the tokens with F1 above 1000Hz.

These analyses are available for view in the project repository. We will talk 
through a few examples here.

![](../images/UXX8_Z6244PSW_T4_ON_20211119_OA5SKD2NZQ__96.857-96.977_comparison.png)
These are the analyses considered for the token with file name `UXX8_Z6244PSW_T4_ON_20211119_OA5SKD2NZQ__96.857-96.977.wav`. It is one of the
two '1's at the top right of @fig-dress-sample. The winning analysis,
the first analysis, is surrounded by a thick black box. The yellow dots and line
indicate F1 and the green indicates F2. We see that the first analysis is 
collapsing F1 and F2 together and the putative F3 (the blue line) is a more
plausible F2. If we look at the alternative analyses, we see that there is a 
higher frequency analysis which produces a better F1 and F2 (reliably from
around the 10th analysis, reading left-to-right and top-to-bottom). As noted above, 
we can encourage FastTrack towards these alternative analyses by setting a 
hard limit on F1 and F2 values.

We now look at the token which appears with '18' at the bottom right of Figure
(@fig-dress-sample). 

![](../images/AV6E_NCFLYZ5I_T4_ON_20211117_0N64QRXC1T__91.734-91.905_comparison.png)
This is a token from the word 'fell', which comes before a liquid, and is 
unsurprisingly, not much like a traditional [dress]{.smallcaps} vowel. The 
analysis picked by FastTrack looks OK.

The token in question, outside the context of the word, is the following:
<audio controls> <source src="../audio/AV6E_NCFLYZ5I_T4_ON_20211117_0N64QRXC1T__91.734-91.905.wav" type="audio/wav">
</audio>

This kind of case will be filtered out below when we remove all tokens
preceding liquids.

Manual inspection of the other cases doesn't reveal obvious problems.

There were eight tokens which could not be tracked by FastTrack. These were:
```{r}
# A function to generate summary tables.
untracked_summary <- function(vowel_name) {
  fasttrack_vowels |> 
    filter(
      vowel == vowel_name,
      !tracked
    ) |> 
    select(token_dur, meanPitch, word, noise)  
}

untracked_summary("DRESS")
```

These were all very short (see left column `token_dur` for duration in seconds),
with no obvious pattern in word or pitch. They did not occur in particularly
noisy portions.

**Decision:** set a lower bound on [dress]{.smallcaps} F2 at 1500 and upper 
bound of 4000.

##### FLEECE


```{r}
#| label: fig-fleece-sample
#| fig-cap: "FLEECE tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "FLEECE"
  ) |> 
  plot_vs_ft()
```

The majority of these values are not in strange places for a small child's
[fleece]{.smallcaps} vowel. 

The token outside the ellipse and marked '2' is a classic case of an F2 being
fit through empty space:

![](../images/F5CI_1G75P3TV_T4_ON_20211116_R4GINAN4H8__62.533-62.676_winner_.png)
In this case, analyses with a higher cut off fix the problem.

A more problematic case is `KSGT_QACKW1N6_T1_ON_20210611_KNOT605KHV__48.485-48.706`. 
The winning analysis is:

![](../images/KSGT_QACKW1N6_T1_ON_20210611_KNOT605KHV__48.485-48.706_winner_.png)

Here, the F2 line (green) is again, going through empty space. However, it is 
going through at a value plausible for an F2. It looks like there is not much 
information in the audio. We enact a bandwidth-based filtering approach below 
and will check whether this token is excluded by it.

The token outside the ellipse which is labelled '5' is a token of 'the' which
sounds, on listening, to be an instance of the [strut]{.smallcaps} vowel which
has been incorrectly categorised in the course of forced alignment. The
appearance of this token well outside the ellipse does not indicate a failure of
formant tracking.

Finally, the token `ILA_T1_SX1LTCO7_SR_30072020__36.435-36.652` is an 
interesting case where the putative F2 is at a plausible value for a [fleece]{.smallcaps} F2, but is too low in the spectrogram. The correct 
analysis doesn't appear until the cut off is around 8000Hz. We have no way to
avoid this kind of problem. It seems to occur in a small minority of cases 
in this sample. We will also check this token when we institute a 
bandwidth-based filtering step below.

The untracked tokens were:
```{r}
untracked_summary("FLEECE")
```

Each of these had a very short duration.

**Decision:** set a lower bound on [dress]{.smallcaps} F2 at 1500 and upper bound
of 4000.

##### TRAP

```{r}
#| label: fig-trap-sample
#| fig-cap: "TRAP tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "TRAP"
  ) |> 
  plot_vs_ft()
```

The token with very high F1, marked '17', outside the ellipse, has an
acceptable spectrogram. The token outside the ellipse marked '5' is an instance
of a token with a bad start time.

![](../images/999I_HFRCOYXN_T1_ON_20210614_9TZRESGTFU__55.015-55.275_winner_.png)
Without manually rechecking all segments, there is no way to avoid this kind of
error coming up in some cases.

```{r}
untracked_summary("TRAP")
```

One very short token was not tracked.

**Decision:** set a lower bound on F2 of 1200 and an upper bound of 3500.

##### NURSE

```{r}
#| label: fig-nurse-sample
#| fig-cap: "NURSE tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "NURSE"
  ) |> 
  plot_vs_ft()
```

This is a reasonably tight ellipse. Most of the winners look OK.

The token with an F1 above 1000 has some serious problems. But none of the 
alternative analyses are any better. It may be that a lower cut off would 
produce a better analysis. If so, we could categorise [nurse]{.smallcaps}
as an 'other' vowel. We won't do this, but note it as a possible alternative
path.

```{r}
untracked_summary("NURSE")
```

All tokens of [nurse]{.smallcaps} could be tracked.

**Decision:** set 1200 as a lower bound for F2 and an upper bound of 3500.

##### GOOSE


```{r}
#| label: fig-goose-sample
#| fig-cap: "GOOSE tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "GOOSE"
  ) |> 
  plot_vs_ft()
```

Our [goose]{.smallcaps} tokens have a very wide spread.

The token with a very high F1 is `KEN_T2_2CDLJ9EQ_SR_02122020__57.264-57.336`.
Listening on LaBB-CAT reveals that the audio is dominated by the sound of a door
closing, yet this does not appear in the noise tier.

```{r}
untracked_summary("GOOSE")
```

All tokens could be tracked.

**Decision:** set 1000 as a lower bound for F2 and 3500 as the upper bound.

##### KIT

```{r}
#| label: fig-kit-sample
#| fig-cap: "KIT tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "KIT"
  ) |> 
  plot_vs_ft()
```

There are no unambiguous formant tracking errors in this sample.

```{r}
untracked_summary("KIT")
```

All untracked tokens were very short.

**Decision:** set 1250 as a lower bound for F2 and 3500 as the upper bound.

##### THOUGHT

```{r}
#| label: fig-thought-sample
#| fig-cap: "THOUGHT tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "THOUGHT"
  ) |> 
  plot_vs_ft()
```

The ellipse is in the right generate area of the vowel space. Visual inspection
of the winning analyses reveals some common error types. 

Many tokens are pre-liquid, and so will be filtered below. For instance, 
'fall' is frequent in the corpus.

In some cases, we see what looks like F0 being marked as F1. For instance, see
KSGT_STWQV47U_T1_ON_20210615_8OUAEYVRYO__108.462-108.596. 

![](../images/KSGT_STWQV47U_T1_ON_20210615_8OUAEYVRYO__108.462-108.596_winner_.png)

Alternative analyses do better with F2, but continue to put F1 somewhere between
the actual F0 and F1. 

The token with very high F2 is `WES_T1_YXFY5YRO_SR_15072020__35.775-35.955`. It
sounds more like [lot]{.smallcaps}.

**Decision:** set a upper bound on F2 of 2250. Set a lower bound on F1 at 350.

##### LOT

```{r}
#| label: fig-lot-sample
#| fig-cap: "LOT tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "LOT"
  ) |> 
  plot_vs_ft()
```

There is no obvious systematic error which could be corrected in the winning
analyses. When the reading is dubious, it is not clear that there is a better
reading available.

The token in the top right marked '6' does sound more like [thought]{.smallcaps}
when listened to. The winning analyses makes the F2 much higher than it should be
at the start of the token but the midpoint is fine.

HAW_T2_X6NPFKXX_SR__156.155-156.307_winner_.png looks like it passes through 
empty space and, when listened to, is barely audible. We will check whether 
this is filtered out by the bandwidth filter.

```{r}
untracked_summary("LOT")
```

Seven tokens were not trackable due to their length.

**Decision:** Set an upper bound on F2 of 2500.

##### FOOT

```{r}
#| label: fig-foot-sample
#| fig-cap: "FOOT tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "FOOT"
  ) |> 
  plot_vs_ft()
```

There are no obvious systematic errors in the winning analyses.

```{r}
untracked_summary("FOOT")
```

There were eight untrackable tokens in the sample.

##### START

```{r}
#| label: fig-start-sample
#| fig-cap: "START tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "START"
  ) |> 
  plot_vs_ft()
```

There is a very wide spread of both F1 and F2 values in this sample. Manual 
inspection shows a fairly typical range of errors. 

The token `HAW_T2_BBF9Y0IL_SR__107.098-107.198` shows the kind of problem which
can occur when F1 and F2 are very close:

![](../images/HAW_T2_BBF9Y0IL_SR__107.098-107.198_comparison.png)

There is no good analysis here, presumably because the real F1 and F2 are both
somewhere between 100 and 1500Hz.

One question with [start]{.smallcaps} is whether `Tama` is being produced with
[start]{.smallcaps} or [trap]{.smallcaps}. Are the instances with [trap]{.smallcaps}
F2 above 2000Hz in this sample all instances of 'Tama'?

```{r}
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "START",
    f2 > 2000
  ) |> 
  select(
    file, word
  )
```

It may be wise to remove 'Tama' tokens if analysing [start]{.smallcaps}. This
vowel is not analysed in this paper, so we leave this as a note for future
projects.

```{r}
untracked_summary("START")
```
Five very short tokens were excluded.

**Decision:** Set lower bound on F2 at 900.

##### STRUT

```{r}
#| label: fig-strut-sample
#| fig-cap: "STRUT tokens in sample. Numbers indicate which FastTrack analysis won."
fasttrack_vowels |> 
  filter(
    tracked,
    vowel == "STRUT"
  ) |> 
  plot_vs_ft()
```

There are some extreme outliers here.

The token marked '22' at the left of the image is 
`AV6E_ZCBCIU5R_T1_ON_20210623_ZFICT8NP0J__103.079-103.159` and is an instance
of a formant being drawn through empty space. It is likely to be filtered
out either by the standard deviation filter or the bandwidth filter. We will
check this below.

The very low F1, marked '16' at the top of the plot, is an instance were there 
is a very faint line which might be the F1 at around 700Hz. The token is
`HAW_T2_FLAGGED_KXU13BOU_SR__61.462-61.595`. None of the alternative analyses
seem to pick this up, so it will not be fixed by setting a hard bound on 
formants. We will check whether it is filtered out at the end of the filtering 
stages.

**Decision:** Set lower bound on F2 at 900.

##### SCHWA

We won't manually check [schwa]{.smallcaps}.

:::

### Apply FastTrack to all tokens

In order to apply the settings we have determined above, we must first extract
the audio for each token. This requires all of the sounds to be in a `sounds`
folder and a `file_information.csv` file which tells FastTrack about each file
(in particular, it says which vowel category a sound belongs to so the correct
limits can be applied.

The formant bounds determined above are, as a table:

| Vowel |	F1 (lower) | F2 (upper)	| F2 (lower) |	F2 (upper) |
|-------|------------|------------|------------|-------------|
| START |	350 |	1500 |	900 |	3500 |
| THOUGHT |	350 |	1500 |	500 |	2250 |
| TRAP |	350 |	1500 |	1200 |	3500 |
| NURSE |	350 |	1500 |	1200 |	3500 |
| DRESS |	350 |	1500 |	1500 |	4000 |
| FLEECE |	350 |	1500 |	1500 |	4000 |
| KIT |	350 |	1500 |	1250 |	3500 |
| LOT |	350 |	1500 |	500 |	2500 |
| GOOSE |	350 |	1500 |	1000 |	3500 |
| FOOT |	350 |	1500 |	500 |	3500 |
| STRUT |	350 |	1500 |	900 |	3500 |

We will also increase the minimum cut off for the back vowels to 5500Hz rather
than 5000Hz.

Note that we set the same bounds for F1 across all vowels and add some 
additional values beyond those specified above.

These are applied to the mean value of the selected analysis. This means that
the midpoint formant reading can fall outside of the bounds we specify. We
decided not to attempt to modify the FastTrack code to apply the formant value
limits to the midpoint.

Local processing needs to be done for each vowel type. The resulting directory
structure is:

- local processing directory.
  - `Front` 
    - `sounds`
      - Sound files here.
    - `file_information.csv`
  - `Back`
    - `sounds`
      - Sound files here.
    - `file_information.csv`
  - `Other`
    - `sounds`
      - Sound files here.
    - `file_information.csv`

The following code block creates this structure and downloads the data.

```{r}
#| eval: false
#| code-fold: true

# It is likely you will want to process these outside of your project directory.
# use the `path_to_local_processing` variable to store the path to your local
# processing directory.
path_to_local_processing <- '/home/jwb/Documents/kids_local_processing/' # Modify this string.

# Establish front, back and mid categories.
vowels <- vowels |> 
  mutate(
    vowel_type = case_when(
      vowel %in% c('FLEECE', 'DRESS', 'NURSE', 'GOOSE', 'TRAP', 'KIT') ~ "Front",
      vowel %in% c('LOT', 'THOUGHT', 'FOOT') ~ "Back",
      .default = "Other"
    )
  )

# Create directory structure
for (i in c("Front", "Back", "Other")) {
  dir.create(
    here(path_to_local_processing, i, "sounds"), 
    recursive = TRUE
  )
}

# Starting nzilbb.labbcat within 'map' doesn't always trigger authentication so we 
# call getLayerIds here as a way to ensure we are authenticated.
getLayerIds(labbcat.url)

# Extract audio files.
match_extraction <- vowels |> 
  group_by(vowel_type) |> 
  nest() |> 
  mutate(
    extracted_files = map2(
      data, 
      vowel_type,
      ~ getSoundFragments(
        labbcat.url,
        ids = .x$transcript,
        start.offsets = .x$start - 0.025,
        end.offsets = .x$end - 0.025,
        path = paste0(path_to_local_processing, .y, "/sounds")
      )
    )
  )

match_extraction <- match_extraction |> 
  mutate(
    data = map2(
      data, 
      extracted_files, 
      ~ mutate(
        .x, 
        file = str_extract(.y, "[A-Za-z0-9_\\.\\-]*$")
      )
    ),
    file_info = map(
      vowel_type, 
      ~ makefileinformation(paste0(path_to_local_processing, .x))
    )
  )


match_extraction <- match_extraction |> 
  mutate(
    file_info = map2(
      file_info,
      data,
      ~ .x |>
        left_join(
          .y |> 
            select(file, vowel)
        ) |> 
        select(-label) |> 
        rename(
          label = vowel
        ) |> 
        relocate(number, file, label)
    )
  )

# save file info
walk2(
  match_extraction$file_info,
  match_extraction$vowel_type, 
  ~write_csv(
    .x, 
    paste0(
      path_to_local_processing,
      .y,
      "/file_information.csv"
    )  
  )
)

# Need to link each token with its audio file.
token2file <- match_extraction |> 
  select(vowel_type, data) |> 
  unnest(data) |> 
  select(MatchId, vowel_type, file)

write_rds(token2file, here('data', 'token2file.rds'))
```

We now run FastTrack on each vowel type separately for each vowel type as we
did in the previous sections to determine the settings to use.

We load the winning formant tracks and take their midpoint with the following
code block.

```{r}
#| eval: false

formants <- tibble(
  vowel_type = c("Back", "Front", "Other")
)

formants <- formants |> 
  mutate(
    file_list = map(
      vowel_type,
      ~ list.files(
          here(path_to_local_processing, .x, 'formants_winners'), 
          full.names=T
        )
    )
  ) |> 
  unnest(file_list)

get_midpoint_values <- function(formant_path) {
  f <- formant.read(formant_path)
  # find midpoint
  f_mid <- round(f$nx/2)
  # get f1, f2 and f3 values at midpoint + bandwidths for each
  formant_values <- c(
    f$frame[[f_mid]]$frequency[1:3],
    f$frame[[f_mid]]$bandwidth[1:3]
  )
}

# get midpoints
formants <- formants |> 
  mutate(
    formant_values = map(file_list, get_midpoint_values)
  )

formants <- formants |> 
  unnest_wider(formant_values, names_sep = "F")

formants <- formants |> 
  rename(
    F1_50 = formant_valuesF1,
    F2_50 = formant_valuesF2,
    F3_50 = formant_valuesF3,
    F1_band = formant_valuesF4,
    F2_band = formant_valuesF5,
    F3_band = formant_valuesF6
  )

# Load the correspondence between tokens and flies.
token2file <- read_rds(here('data', 'token2file.rds'))

# join file with vowels.
vowels <- vowels |> 
  left_join(
    token2file
  )

formants <- formants |> 
  mutate(
    # Remove full path.
    file = str_extract(file_list, '[A-Za-z0-9_\\.\\-]*\\.Formant$'),
    # Change suffix
    file = str_replace(file, '_winner_\\.Formant', '\\.wav'),
  )

vowels <- vowels |> 
  left_join(
    formants |> 
      select(file, F1_50, F2_50, F3_50, F1_band, F2_band, F3_band)
  )

write_rds(vowels, here('data', 'tracked_vowels.rds'))
```

```{r}
#| include: false
vowels <- read_rds(here('data', 'tracked_vowels.rds'))
# Get this in the right place!
vowels <- vowels |> 
    mutate(
        ethnicity = if_else(ethnicity == "MÄori", "Māori", ethnicity)
    )
```

Note that we have taken F3 measurements as part of our dataset. We
don't use them in the analysis, but they may be useful for other researchers
and for post-hoc analysis.

### Validate settings {#sec-validation}

We again take a sample of tokens, and look to see how well the current settings
perform. This time, unlike in Section (@sec-vowel-by-vowel), we create a list
of tokens to look at from the local processing directory and copy them across
to a validation directory within the project folder. This is shared in the
project repository.


```{r}
#| code-fold: true
#| eval: false

validation_samples <- vowels |> 
  slice_sample(
    by = "vowel", n = 25
  )

# We save information about which tokens are in the samples.
write_rds(validation_samples, here('data', 'validation_samples.rds'))

# Some code is needed to gather these samples up.

# generate image name. 
validation_samples$file[[1]]
str_replace(validation_samples$file[[1]], '\\.wav', '_winner_.png')


# Create location for the validation samples
for (i in unique(vowels$vowel)) {
  dir.create(
    here('validation', i), 
    recursive = TRUE
  )
}

# transfer image files.
for (i in seq_along(validation_samples$file)) {
  file.copy(
    from = paste0(
      path_to_local_processing, 
      validation_samples$vowel_type[[i]],
      "/images_winners/",
      str_replace(validation_samples$file[[i]], '\\.wav', '_winner_.png')
    ),
    to = here(
      "validation",
      validation_samples$vowel[[i]],
      str_replace(validation_samples$file[[i]], '\\.wav', '_winner_.png')
    )
  )
}

```

We again use a collection of tabs to see the results for each vowel.

::: {.callout-note}
In each of the tabs below, you can click on the plots to enlarge them.
:::

::: {.panel-tabset}

#### DRESS

```{r}
#| lightbox:
#|   group: dress-images
#| cache: true
#| layout-ncol: 5

plot_validation_images <- function(vowel) {
  images <- list.files(here('validation', vowel), full.names = TRUE)
  
  for (image in images) {
    to_plot <- load.image(image)
    plot(to_plot, main = str_extract(image, '[A-Za-z0-9\\.\\-\\_]*$'))
  }
}

plot_validation_images("DRESS")
```

#### FLEECE

```{r}
#| lightbox:
#|   group: fleece-images
#| cache: true
#| layout-ncol: 5

plot_validation_images("FLEECE")
```


#### TRAP

```{r}
#| lightbox:
#|   group: trap-images
#| cache: true
#| layout-ncol: 5
#| 
plot_validation_images("TRAP")
```

#### NURSE


```{r}
#| lightbox:
#|   group: nurse-images
#| cache: true
#| layout-ncol: 5
#| 
plot_validation_images("NURSE")
```

#### GOOSE

```{r}
#| lightbox:
#|   group: goose-images
#| cache: true
#| layout-ncol: 5

plot_validation_images("GOOSE")
```

#### KIT

```{r}
#| lightbox:
#|   group: kit-images
#| cache: true
#| layout-ncol: 5

plot_validation_images("KIT")
```

#### THOUGHT

```{r}
#| lightbox:
#|   group: thought-images
#| cache: true
#| layout-ncol: 5

plot_validation_images("THOUGHT")
```

#### LOT

```{r}
#| lightbox:
#|   group: lot-images
#| cache: true
#| layout-ncol: 5

plot_validation_images("LOT")
```

#### FOOT

```{r}
#| lightbox:
#|   group: foot-images
#| cache: true
#| layout-ncol: 5

plot_validation_images("FOOT")
```

#### START

```{r}
#| lightbox:
#|   group: start-images
#| cache: true
#| layout-ncol: 5

plot_validation_images("START")
```

#### STRUT

```{r}
#| lightbox:
#|   group: strut-images
#| cache: true
#| layout-ncol: 5

plot_validation_images("STRUT")
```

#### SCHWA

```{r}
#| lightbox:
#|   group: schwa-images
#| cache: true
#| layout-ncol: 5

plot_validation_images("SCHWA")
```

:::

While there are some errors in the formant tracks above, they are looking 
good overall. **NB:** we are only using F1 and F2 in this project.

# Filtering and New Variables

We load the tracked formants. 

```{r}
# Set up vowels as a factor
vowels <- vowels |> 
  mutate(
    vowel = as.factor(vowel)
  )

# Variables with names ending '_count' will be used at the end of the markdown
# to plot the filtering process.
initial_count <- nrow(vowels)
initial_count
```

We output the plot in @fig-all-default again on the left and the 
results of locally processing the tokens on the right. 

```{r}
#| label: fig-all-alt
#| fig.cap: |
#|   All monophthong tokens, with default FastTrack settings (right) 
#|   and modified settings (right). 

ft_modified_plot <- vowels |>
  # remove fast track default data.
  select(
    -F1, -F2
  ) |> 
  # replace with locally tracked values
  rename(
    F1 = F1_50,
    F2 = F2_50
  ) |> 
  plot_vs(vowel_colours=vowel_colours) +
  labs(
    title = "F1 and F2 for All Monophthongs",
    subtitle = "Local processing, 1 s.d. ellipses.",
    colour = "Vowel"
  )

combined_ft_plot <- ft_default_plot + ft_modified_plot +
  plot_layout(guides = 'collect')
combined_ft_plot
```
@fig-all-alt is not incredibly revealing. However, we do see some desired
tightening of the F2 ranges for many of these vowels.

We now move on to a series of filtering steps.

## Demographic Factors

We remove speakers who do not have English amongst their home languages.

The possible values for `home_language` are: `r unique(vowels$home_language)`.
We select those values which include English.

```{r}
vowels <- vowels |> 
  filter(
    str_detect(home_language, "English")
  )

language_count <- nrow(vowels)
```


## Linguistic Factors: Stress, Stopwords, and Following Environment

Rather than filtering the bulk of our data as in [@Brand_2021], we create a 
series of variables with a related to systematic errors in formant tracking
and which which can be incorporated into our models.

For instance, instead of removing all stop words
or unstressed variables, we will allow for the effect of these linguistic
phenomena to be controlled in our models.

We do this because data is at a premium in this study. We have very low token
counts for many of the children in the corpus as compared with corpora of adult
speech.

We track unstressed vowels, but remove tokens of 
<span style="font-variant: small-caps;">schwa</span> and tokens from the words
"hana", "hanas", and "hana's" as they are variably produced with Māori
pronunciation ([start]{.smallcaps}) or as in "Hannah" ([trap]{.smallcaps}).

```{r}
vowels <- vowels |>
  mutate(
    stressed = stress != "0"
  ) |> 
  filter(
    vowel != "SCHWA",
    !word %in% c("hana", "hanas", "hana's")
  )

word_count <- nrow(vowels)
word_count
```

We remove tokens without orthography and words containing hesitations. We 
create a variable to indicate whether a word is a stop word.

```{r}
vowels <- vowels |> 
    filter(
      !is.na(word),
      !str_detect(word, "~")
    )

stopwords <- c(
  # List from Brand 2021.
  'a', 'ah', 'ahh', 'am', 'an', 'and', 'are', "aren't", 'as', 'at',
  'aw', 'because', 'but', 'could', 'do', "don't", 'eh', 'for', 'from', 'gonna',
  'had', 'has', 'have', 'he', "he's", 'her', 'high', 'him', 'huh', 'i', "i'll",
  "i'm", "i've", "i'd", 'in', 'into', 'is', 'it', "it's", 'its', 'just', 'mean',
  'my', 'nah', 'not', 'of', 'oh', 'on', 'or', 'our', 'says', 'she', "she's",
  'should', 'so', 'than', 'that', "that's", 'the', 'them', 'there', "there's",
  'they', 'this', 'to', 'uh', 'um', 'up', 'was', "wasn't", 'we', 'were', 'what',
  'when', 'which', 'who', 'with', 'would', 'yeah', 'you', "you've",
  # Additional identified stop words.
  "then", "me", "too", "his",  "off", "onto", "can't", "can", "cos", "said", 
  "where")

vowels <- vowels |> 
    mutate(
      stopword = word %in% stopwords
    )

na_word_count <- nrow(vowels)
# 12140
```


We create a variable to track following segment category and remove tokens
preceding liquids.
```{r}
vowels <- vowels |> 
  mutate(
    following_segment_category = fct_collapse(
      fct_na_value_to_level(as_factor(following_segment)),
      labial = c('m', 'p', 'b', 'f', 'w'),
      velar = c('k', 'g', 'N'),
      liquid = c('r', 'l'),
      final = c(NA),
      other_level = "other"
    )
  )

vowels <- vowels |> 
  filter(
    following_segment_category != 'liquid'
  )

liquid_count <- nrow(vowels)
# 11406
```


## Formant Tracking Errors

We filter out untracked tokens. These are typically untracked due to being
too short for FastTrack to function. Examples of this were shown above in 
@sec-vowel-by-vowel.

```{r}
vowels <- vowels |> 
  filter(
    !is.na(F1_50)
  )

na_count <- nrow(vowels)
# 9505
```

Tokens were annotated for background noise. We remove any token which falls in
an identified noisy section of audio.
```{r}
vowels <- vowels |> 
  filter(
    is.na(noise)
  )

noise_count <- nrow(vowels)
```

We now carry out a bandwich based filtering step. The bandwidth represents the
span of frequencies with amplitude within +- 3dB of the identified peak. This is
particularly important in situations where a non-existent formant is picked out.
We've seen a couple of examples of this in our initial determination of
FastTrack settings and in our validation of our formant tracking settings (
again, see @sec-vowel-by-vowel). In cases, as in <span style="font-variant:
small-caps;">fleece</span> tokens with a very high actual F2, we sometimes end
up with a putative F2 being traced through the more-or-less empty space in the
spectrogram between F1 and the (very high) F2. In cases like this, the formant
bandwidth should be very high. On the other hand, very small bandwidths are
likely when certain kinds of noise are being tracked rather than speech.

First, let's look at the distribution of bandwidths for F1 and  F2.

```{r}
vowels |> 
  select(
    -F3_band
  ) |> 
  pivot_longer(
    cols = contains('_band'),
    values_to = 'bandwidth',
    names_to = 'formant_type'
  ) |> 
  mutate(
    formant_type = str_replace(formant_type, '_band', '')
  ) |>
  ggplot(
    aes(
      x = bandwidth,
      colour = formant_type
    )
  ) +
  geom_freqpoly(linewidth=1, stat="density") +
  xlim(xlim=c(0, 2000)) +
  labs(
    title = "Formant Bandwidths"
  )
```

These have very large right tails, along with a sizable number of tokens with 
very low bandwidths (although this is harder to see).

Let's look at some examples. First, we define some variables which contain
tokens with bandwidth above 500Hz (high bandwidth) and tokens with bandwidth
below 10Hz (low bandwidth).
```{r}
set.seed(500)
high_f1_bandwidth <- vowels |> 
  filter(
    F1_band > 500
  ) |> 
  slice_sample(n=10)

high_f2_bandwidth <- vowels |> 
  filter(
    F2_band > 500
  ) |> 
  slice_sample(n=10)


low_f1_bandwidth <- vowels |> 
  filter(
    F1_band < 10
  ) |> 
  slice_sample(n=10)

low_f2_bandwidth <- vowels |> 
  filter(
    F2_band < 10
  ) |> 
  slice_sample(n=10)

```

We now load some example images. The following block copies across the relevant
images from the local processing folder (and so requires access to this).
``` {r}
#| eval: false
band_groups <- c("high_f1", "high_f2", "low_f1", "low_f2")

# Create location for the validation samples
for (i in band_groups) {
  dir.create(
    here('bandwidth', i), 
    recursive = TRUE
  )
}

# transfer image files.
transfer_images <- function(in_data, in_group) {

  for (i in seq_along(in_data$file)) {
    file.copy(
      from = paste0(
        path_to_local_processing, 
        in_data$vowel_type[[i]],
        "/images_winners/",
        str_replace(in_data$file[[i]], '\\.wav', '_winner_.png')
      ),
      to = here(
        "bandwidth",
        in_group,
        str_replace(in_data$file[[i]], '\\.wav', '_winner_.png')
      )
    )
  }
}

transfer_images(high_f1_bandwidth, "high_f1")
transfer_images(high_f2_bandwidth, "high_f2")
transfer_images(low_f1_bandwidth, "low_f1")
transfer_images(low_f2_bandwidth, "low_f2")
```


``` {r}
#| lightbox:
#|   group: high-f1-band
#| cache: true
#| layout-ncol: 5
plot_winner <- function(vowel_type, in_group, audio_filename, bandwidth) {
  to_plot <- load.image(
    here(
      'bandwidth',
      in_group,
      str_replace(audio_filename, '\\.wav', '_winner_.png')
    )
  )
  plot(
    to_plot, 
    main = paste(
      'Bandwidth:', bandwidth, 'Hz\n', 
      vowel_type, str_extract(audio_filename, '[A-Za-z0-9\\.\\-\\_]*$')
    ),
    axes = FALSE
  )
}

for (i in seq_along(high_f1_bandwidth$file)) {
  plot_winner(
    high_f1_bandwidth$vowel_type[[i]],
    'high_f1',
    high_f1_bandwidth$file[[i]],
    round(high_f1_bandwidth$F1_band[[i]])
  )
}
```

When looking at these it is important to note that the formant and bandwidth
readings we are using are midpoint readings.

These are _high bandwidth_ tokens for F1. We are looking to confirm that these
are indeed _bad readings_. We are looking at the yellow lines at the midpoint, 
indicated by the dashed vertical line.

There are a few different error types here. In some cases, we have very wide,
thick, black bars in which it is difficult to discern F1 as distinct from F0
and F2 (e.g. `AV6E_2CDLJ9EQ_...`). We also have the opposite problem (wide empty
spaces) in other tokens (`HAW_T1_BBF9Y0IL...` and `HAW_T1_BBF9Y0IL...`).
There are other, quite chaotic tokens (e.g. `P3NP_WE3NU3LF_T4...`). Some tokens
look OK, such as `HAW_T2_JPEJCXCI_...`. However, the bandwidth on this token 
is in fact higher than that of tokens which look bad (e.g. `ECA5_TKAB8B7Y...`).
This is likely unavoidable (as is often the case when filtering data). 

```{r}
#| lightbox:
#|   group: high-f2-band
#| cache: true
#| layout-ncol: 5
for (i in seq_along(high_f2_bandwidth$file)) {
  plot_winner(
    high_f2_bandwidth$vowel_type[[i]],
    'high_f2',
    high_f2_bandwidth$file[[i]],
    round(high_f2_bandwidth$F2_band[[i]])
  )
}
```

We now switch to the lines of green points.

It looks like 500Hz is too low for our F2 cut off. The obvious errors being
picked up here (including, for instance, `SFH9_TYZA8SPF...`) seem to have 
bandwidths above 1000Hz. 

What happens at the other side of the distribution? i.e. what do tokens with 
_very low_ bandwidth look like?

```{r}
#| eval: false 
for (i in seq_along(low_f1_bandwidth$file)) {
  plot_winner(
    low_f1_bandwidth$vowel_type[[i]],
    'low_f1',
    low_f1_bandwidth$file[[i]],
    round(low_f1_bandwidth$F1_band[[i]])
  )
}
```

There's nothing obviously wrong with these low bandwidth F1 tokens visually with
the exception of `F5CI_S29WR30F...`.

```{r}
#| eval: false 
for (i in seq_along(low_f2_bandwidth$file)) {
  plot_winner(
    low_f2_bandwidth$vowel_type[[i]],
    'low_f2',
    low_f2_bandwidth$file[[i]],
    round(low_f2_bandwidth$F2_band[[i]])
  )
}
```

The token `AV6E_2CDLJ9EQ` is alarming. There seems to be a non-speech sound
happening and not much otherwise.

In the absence of a principled cut off point, we will simply track whether the
bandwidth of a token is in the top 10% of the distribution for F1, F2, and F3 
bandwidths.

The cut off values are:

```{r}
vowels |> 
  summarise(
    F1_band = quantile(F1_band, 0.90),
    F2_band = quantile(F2_band, 0.90),
    F3_band = quantile(F3_band, 0.90)
  )
```

We make three variables to track this. In order to keep good F1 readings, 
even if the F2 reading is bad, we will separate our data into F1 and F2 datasets
at the analysis stage. At that point, we will filter out the tokens with 
high bandwidths. But it is important at this stage to quantify how many tokens
will be lost. 

```{r}
vowels <- vowels |> 
  mutate(
    high_f1_bandwidth = F1_band > quantile(F1_band, 0.90),
    high_f2_bandwidth = F2_band > quantile(F2_band, 0.90),
    high_f3_bandwidth = F3_band > quantile(F3_band, 0.90)
  )

f1_band_count <- nrow(filter(vowels, !high_f1_bandwidth))
f2_band_count <- nrow(filter(vowels, !high_f2_bandwidth))

# If these counts aren't the same, something has gone wrong!
f1_band_count == f2_band_count
```

Since we are taking the top 10% of tokens, each of these variables 
identifies `r nrow(filter(vowels, high_f1_bandwidth))` tokens.

Phonetic research which deploys automated formant tracking often filters out
tokens above or below 2.5 standard deviations from the mean _within speaker_. 
This is not possible for us, as there are not enough tokens per speaker. The
distribution of tokens for each speaker and collection point is as follows:

```{r}
vowels |> 
  group_by(participant, collect) |> 
  count(vowel) |> 
  ggplot(
    aes(
      x = n,
      colour = vowel
    )
  ) +
  geom_freqpoly(bins = 15)
```

Instead of taking each speaker individually, we will work out standard
deviations within age bins.

We use three month bins, starting from 46-48 months, and ending with 61 months
and above.

```{r}
vowels <- vowels |> 
  mutate(
    age_bin = case_when(
      between(age, 46, 48) ~ "46-48",
      between(age, 49, 51) ~ "49-51",
      between(age, 52, 54) ~ "52-54",
      between(age, 55, 57) ~ "55-57",
      between(age, 58, 60) ~ "58-60",
      between(age, 61, 70) ~ "61+"
    ) 
  )
```

We also track a standard deviation limit for a set of 'age bins'. 
Because speakers
do not have many tokens each, we apply sd filtering at the level of age bins. 
We will use a cut off of 2 standard deviations.

```{r}
sd_var <- function(in_data, sd_limit = 2.5) {
  vowels_all_summary <- in_data %>%
    # Remove tokens at +/- sd limit (default 2.5 sd)
    group_by(age_bin, vowel) %>%
    summarise(
      #calculate the summary statistics required for the outlier removal.
      n = n(),
      mean_F1_50 = mean(F1_50, na.rm = TRUE),
      mean_F2_50 = mean(F2_50, na.rm = TRUE),
      mean_F3_50 = mean(F3_50, na.rm = TRUE),
      sd_F1_50 = sd(F1_50, na.rm = TRUE),
      sd_F2_50 = sd(F2_50, na.rm = TRUE),
      sd_F3_50 = sd(F3_50, na.rm = TRUE),
      # Calculate cut off values.
      max_F1_50 = mean(F1_50) + sd_limit*(sd(F1_50)),
      min_F1_50 = mean(F1_50) - sd_limit*(sd(F1_50)),
      max_F2_50 = mean(F2_50) + sd_limit*(sd(F2_50)),
      min_F2_50 = mean(F2_50) - sd_limit*(sd(F2_50)),
      max_F3_50 = mean(F3_50) + sd_limit*(sd(F3_50)),
      min_F3_50 = mean(F3_50) - sd_limit*(sd(F3_50))
    )

  #this is the main outlier filtering step.
  out_data <- in_data %>%
    left_join(vowels_all_summary) %>%
    mutate(
      F1_outlier = ifelse(
        F1_50 > min_F1_50 &
          F1_50 < max_F1_50,
        FALSE, 
        TRUE
      ),
      F2_outlier = ifelse(
        F2_50 > min_F2_50 &
          F2_50 < max_F2_50, 
        FALSE, 
        TRUE
      ),
      F3_outlier = ifelse(
        F3_50 > min_F3_50 &
          F3_50 < max_F3_50, 
        FALSE, 
        TRUE
      )
    ) %>%
    ungroup() |> 
    select(
      -c(
        "mean_F1_50", "mean_F2_50", "mean_F3_50",
        "sd_F1_50", "sd_F2_50", "sd_F3_50",
        "max_F1_50", "min_F1_50", 
        "max_F2_50", "min_F2_50",
        "max_F3_50", "min_F3_50"
      )
    )
}
```


We apply the fucntion to generate the outlier columns.
``` {r}
vowels <- sd_var(vowels, sd_limit = 2)
```

We check that this function works by looking at the distribution of
[dress]{.smallcaps} F1 in each age bin.
```{r}
vowels |> 
  filter(
    vowel == "DRESS"
  ) |> 
  ggplot(
    aes(
      x = F1_50,
      fill = F1_outlier
    )
  ) +
  geom_histogram() +
  facet_grid(~ age_bin)
```
This looks about right. Most values identified appear on the right tail of the
distributions.

Now we look at F2:

```{r}
vowels |> 
  filter(
    vowel == "DRESS"
  ) |> 
  ggplot(
    aes(
      x = F2_50,
      fill = F2_outlier
    )
  ) +
  geom_histogram() +
  facet_grid(~ age_bin)
```

Here, the identified values tend to be on the left tail of the distribution.

We quantify how many outliers are identified. We also exclude high bandwidth
tokens here, for the purpose of quantifying the filtering process below.

```{r}
f1_sd_count <- nrow(
  vowels |> 
    filter(!high_f1_bandwidth, !F1_outlier)
)

f2_sd_count <- nrow(
  vowels |> 
    filter(!high_f2_bandwidth, !F2_outlier)
)

f3_sd_count <- nrow(
  vowels |> 
    filter(!high_f3_bandwidth, !F3_outlier)
)
```

There are `r nrow(vowels) - f1_sd_count` F1 outliers detected, `r nrow(vowels) - f2_sd_count` F2 outliers,
and `r nrow(vowels) - f3_sd_count` F3 outliers.

We now track vowels with high F0. Sometimes, F0 sits on top of F1 and makes it
hard to distinguish between the two.

Let's look at the distribution of F0.

```{r}
vowels <- vowels |> 
  mutate(
    meanPitch = as.numeric(meanPitch)
  )

vowels |> 
  select(
    F1_50, F2_50, meanPitch
  ) |> 
  pivot_longer(
    cols = c("F1_50", "F2_50", "meanPitch"),
    names_to = "type",
    values_to = "value"
  ) |> 
  ggplot(
    aes(
      x = value,
      colour = type
    )
  ) +
  geom_freqpoly()
```

As expected, there's a substantial overlap between the F0 and F1 distribution.

We create a variable to track the top 10% of tokens by F0.
```{r}
vowels <- vowels |> 
  mutate(
    high_pitch = meanPitch > quantile(vowels$meanPitch, 0.90, na.rm = T)
  )
```

Finally, we filter out extreme values for F1.

```{r}
vowels <- vowels |> 
  filter(F1_50 < 1500 & F1_50 > 350)

extreme_count <- nrow(vowels)
# 9289

f1_extreme_count <- f1_sd_count - (na_count - extreme_count)
f2_extreme_count <- f2_sd_count - (na_count - extreme_count)
```

We're left with `r nrow(vowels)` tokens.

## Check Previously Identified Tokens

We identified four tokens to check on in @sec-validation.

```{r}
vowels |> 
  filter(
    file %in% c(
    "UXX8_Z6244PSW_T4_ON_20211119_OA5SKD2NZQ__96.857-96.977.wav",
		"ILA_T1_SX1LTCO7_SR_30072020__36.435-36.652.wav",
		"AV6E_ZCBCIU5R_T1_ON_20210623_ZFICT8NP0J__103.079-103.159.wav",
		"HAW_T2_FLAGGED_KXU13BOU_SR__61.462-61.595.wav"
    )
  ) |> 
  select(
    transcript, participant, collect, F1_50, F2_50, noise, 
    contains('outlier'), contains('band')
  ) |> 
  knitr::kable() |> 
  scroll_box(width="100%")
```

The token `HAW_T2_FLAGGED_KXU13BOU_SR__61.462-61.595.wav` has
been filtered out.

`ILA_T1_SX1LTCO7_SR_30072020.slt` and 
`AV6E_ZCBCIU5R_T1_ON_20210623_ZFICT8NP0J.slt` have been correctly identified
as having very high F2 bandwidth. 

The token `UXX8_Z6244PSW_T4_ON_20211119_OA5SKD2NZQ__96.857-96.977.wav` has
correctly been shifted up in F2 from the FastTrack defaults 
(see @sec-validation).

# Interim Data Description {#sec-data-desc} 

We now give a description of the data before normalisation.

## Vowel Space

First, a simple vowel space plot:

```{r}
vowels |> 
  relocate(
    participant, vowel, F1_50, F2_50
  ) |> 
  plot_vowel_space(
    vowel_colours = vowel_colours,
    means_only=FALSE, 
    facet=FALSE, 
    ellipse=TRUE, 
    point_alpha=0.2
  ) +
  labs(
    title = "Raw Formant Values After Filtering"
  )
```

## Participant Level Summary

We have `r length(unique(vowels$participant))` participants.

How many collections points do the participants have?
``` {r}
participants_collect <- vowels |>
  group_by(participant) |> 
  summarise(
    collection_points = n_distinct(collect)
  ) |> 
  group_by(collection_points) |> 
  summarise(n_participants = n())

participants_collect
```

We have `r participants_collect$n_participants[[3]]` participants at 3 collection points, 
with almost none at 4 collection points, and `r participants_collect$n_participants[[1]]` and 
`r participants_collect$n_participants[[2]]` each at collection points 1 and 2.

The youngest speaker in our data is `r signif(min(vowels$age)/12, 2)` years 
old, and the oldest is `r signif(max(vowels$age)/12, 2)`, with a range
of `r range(vowels$age)[2] - range(vowels$age)[1]`
months.

We look at the gender balance by participant and by token.

``` {r part-gender, fig.cap = "Participants by gender."}
participants_data <- vowels |> 
  group_by(participant) %>%
  summarise(
    gender = first(gender),
    ethnicity = first(ethnicity),
    home_language = first(home_language),
    dob = first(dob)
  )

participants_data |> 
  ggplot(
    aes(
      x = gender
    )
  ) +
  geom_histogram(stat="count") +
  labs(title = "Participants by gender")
```

There are `r nrow(participants_data |> filter(gender == "F"))` 
female and `r nrow(participants_data |> filter(gender == "M"))` 
male speakers.

At the level of tokens, the gender balance is:
``` {r token-gender, fig.cap = "Tokens by gender."}
vowels |> 
  ggplot(
    aes(
      x = gender
    )
  ) +
  geom_histogram(stat="count") +
  labs(title = "Tokens by gender")
```

At the token level, there are slightly more available from female speakers 
than male.

We also have ethnicity and home language information.
``` {r part-ethnicity, fig.cap = "Participant ethnicity"}
participants_data |> 
  ggplot(
    aes(
      x = ethnicity
    )
  ) +
  geom_histogram(stat="count") +
  labs(title = "Participants by ethnicity")
```

Dominance by NZ Europeans. Not too surprising given Christchurch demographics.

``` {r part-language, fig.cap = "Participant home languages"}
participants_data |> 
  ggplot(
    aes(
      x = home_language
    )
  ) +
  geom_histogram(stat="count") +
  labs(title = "Participants by home language") +
  theme(
    axis.text.x = element_text(angle = 90)
  )
```

We decided above to filter by English being one of the particpiants home 
languages.

Finally, the distribution of birthdays:
``` {r participant-birthdays, fig.cap = "Participants by date of birth."}
participants_data |> 
  mutate(
    month_year = ym(str_sub(dob, start = 1L, end = -4L))
  ) |> 
  ggplot(
    aes(
      x = month_year
    )
  ) +
  geom_histogram(stat="count") +
  labs(title = "Participants by date of birth") +
  theme(
    axis.text.x = element_text(angle = 90)
  )
```

## Centre Level Summary

## Centres

How is our data distributed across centres?

``` {r}
centres_data <- vowels |> 
  group_by(transcript_centre) |> 
  summarise(tokens=n(), participants = n_distinct(participant)) |> 
  arrange(desc(participants))
centres_data
```

We have `r nrow(centres_data |> filter(participants >= 10))` centres with 10 
or more participants and `r nrow(centres_data)` centres overall.

We can depict this:
``` {r centres-token}
#| fig-cap: "Centres by token."
centres_data |> 
  ggplot(
    aes(
      x = transcript_centre,
      y = tokens
    )
  ) +
  geom_col()
```

``` {r centres-participant}
#| fig-cap: "Centres by token."
centres_data |> 
  ggplot(
    aes(
      x = transcript_centre,
      y = participants
    )
  ) +
  geom_col()
```

## Tables

How many tokens are stopwords or unstressed?

```{r}
vowels |> 
  count(stopword)
```

Unstressed?

```{r}
vowels |> 
  count(stressed)
```

What range of ethnicity identifications?
```{r}
participants_data |> 
  count(ethnicity)
```

When were the participants born?
```{r}
participants_data |> 
  pull(dob) |> 
  ymd() |> 
  summary()
```

What about the distribution of ages at collection point?
```{r}
pc_data <- vowels |> 
  group_by(participant, collect) %>%
  summarise(
    gender = first(gender),
    ethnicity = first(ethnicity),
    home_language = first(home_language),
    dob = first(dob),
    age = first(age)
  )

pc_data |> 
  pull(age) |> 
  summary()
```


### Data density

In the paper, we make some claims about the difference in the 'density' of the
transcripts. I.e., we talk about the different number of tokens per minute in
adult speech (in this case, QuakeBox data shared via the `nzilbb.labbcat`
function) and child speech.

First, what is the range of tokens per minute in the QuakeBox corpus?

```{r}
qb_vowels |> 
  group_by(
    speaker
  ) |> 
  summarise(
    duration = max(time) / 60,
    tokens = n()
  ) |> 
  mutate(
    tokens_per_minute = tokens / duration
  ) |> 
  pull(tokens_per_minute) |> 
  summary()
```
And now, what is the range of tokens per minute in our pre-school corpus?
```{r}
vowels |> 
  group_by(
    participant, collect
  ) |> 
  summarise(
    duration = max(start) / 60,
    tokens = n(),
    .groups = "keep"
  ) |> 
  mutate(
    tokens_per_minute = tokens / duration
  ) |> 
  pull(tokens_per_minute) |> 
  summary()
```

## Filtering Flow Chart

``` {r}
#| label: filtering-flow
#| fig.cap: "Data filtering flow chart."
#| fig.height: 10
# Flow chart generated using the DiagrammeR package.
flow_chart <- mermaid(
  diagram = glue("
  graph TB
  
  A(Extract data from LaBB-CAT: {initial_count} tokens) --> 
  B(Home language: {language_count} tokens)
  B --> C(SCHWA and HANA: {word_count} tokens)
  C --> D(Stopwords, hesitations, and missing words: {na_word_count} tokens)
  D --> E(Preceeding liquids: {liquid_count} tokens)
  E --> F(FastTrack errors: {na_count} tokens)
  F --> Q(Noisy tokens: {noise_count} tokens)
  Q --> G(F1: high bandwidth: {f1_band_count} tokens)
  Q --> H(F2: high bandwidth: {f2_band_count} tokens)
  G --> I(F1: SD filter: {f1_sd_count} tokens)
  H --> J(F2: SD filter: {f2_sd_count} tokens)
  I --> K(F1: extremes: {f1_extreme_count} tokens)
  J --> L(F2: extremes: {f2_extreme_count} tokens)
  "),
  width = 800
)

flow_chart
```


# Normalisation

We investigated multiple normalisation methods. The two used in the paper are
Lobanov 2.0 on the full vowel space _with imputation_ and Lobanov 2.0 on a 
subset of the data.

We also investigated the use of two log-mean approaches. We do not use these in
the paper, but add them to these supplementary materials as they may be useful
for other researchers.

## Lobanov 2.0 (top 5 vowels)

Lobanov 2.0 requires us to estimate means for each vowel type and a standard
deviation. In Brand et al. 2021, 5 tokens per 
category are required. We don't have sufficient data for 
that for most speakers.

The following function calculates the data loss associated with different 
filtering strategies for applying Lobanov 2.0. We take then top $n$ vowel 
types by frequency and work out how much data remains. We require at least
3 tokens for each vowel type.

```{r}
lob_counts <- tibble(
  vowel_types = seq(11, 1, -1)
)

get_lob_count <- function(vowel_types, in_data) {
  
  in_data <- in_data |> 
    mutate(
      pc = paste0(participant, '-', collect)
    ) 
  
  type_counts <- in_data |> 
    count(vowel) |> 
    arrange(desc(n))
  
  top_n_vowels <- type_counts |> 
    slice_head(n=vowel_types) |> 
    pull(vowel)
  
  pcs_to_filter <- in_data |> 
    filter(vowel %in% top_n_vowels) |> 
    mutate(vowel = droplevels(vowel)) |> 
    group_by(pc, vowel, .drop=FALSE) |> 
    summarise(
      type_count = n(),
      .groups = "drop_last"
    ) |> 
    ungroup() |> 
    group_by(pc) |> 
    mutate(
      min_count = min(type_count)
    ) |> 
    filter(
      min_count < 3
    ) |> 
    pull(pc) |> 
    unique()
  
  filtered_data <- in_data |>
    filter(
      vowel %in% top_n_vowels,
      !pc %in% pcs_to_filter
    ) |> 
    select(-pc)
  
  speaker_count = length(unique(filtered_data$participant))
  token_count = nrow(filtered_data)
  
  out_list <- list(speakers = speaker_count, tokens = token_count)
}

lob_counts <- lob_counts |> 
  mutate(
    counts = map(vowel_types, ~ get_lob_count(.x, vowels))
  ) |> 
  unnest_wider(counts)
```

The results can be visualised in two ways. First, how many speakers are left?
```{r}
lob_counts |> 
  ggplot(
    aes(
      x = vowel_types,
      y = speakers
    )
  ) +
  geom_col() +
  labs(
    title = "Speakers with sufficient data for Lobanov 2.0",
    y = "Speakers",
    x = "Top n vowel types"
  )
```
We see if we insisted on 11 vowels we would have 1 speaker remaining.

Second, we quantify the _tokens_ remaining if we insist on different numbers of
vowel types.

```{r}
lob_counts |> 
  ggplot(
    aes(
      x = vowel_types,
      y = tokens
    )
  ) +
  geom_col() +
  labs(
    title = "Token counts for Lobanov 2.0 with top-n vowels",
    x = "Top n vowels",
    y = "Tokens"
  )
```

Tokens are maximised if we only take the _five_ most frequent vowel types! In
that case, we would be down to sixty-one speakers and 1000 tokens. The five types
are <span style="font-variant: small-caps;">dress</span> and <span style="font-variant: small-caps;">kit</span>.

```{r}
vowels |> 
  count(vowel) |> 
  arrange(desc(n))
```

The top five vowels are <span style="font-variant: small-caps;">KIT, FLEECE, TRAP, STRUT,</span> 
and <span style="font-variant: small-caps;">DRESS</span>. 
We will make a subset of the data and apply Lobanov 2.0 normalisation to it
in the usual fashion. This will be used later as a check to ensure we are not 
being led astray by the imputation approach we will use for the main analysis.

```{r}
# 'pc' is short for 'participant_collect'. We want to control for growth, so
# we're normalising separately for each collection point.
pcs_to_filter <- vowels |> 
  filter(
    vowel %in% c('KIT', 'STRUT', 'DRESS', 'FLEECE', 'TRAP')
  ) |> 
  mutate(
    vowel = droplevels(vowel),
    pc = paste0(participant, '-', collect)
  ) |> 
  group_by(pc, vowel, .drop=FALSE) |> 
  summarise(
    type_count = n()
  ) |> 
  ungroup() |> 
  group_by(pc) |> 
  mutate(
    min_count = min(type_count)
  ) |> 
  filter(
    min_count < 3
  ) |>
  pull(pc) |> 
  unique()
  
lob2_sub_vowels <- vowels |>   
  filter(
    vowel %in% c('KIT', 'STRUT', 'DRESS', 'FLEECE', 'TRAP')
  ) |> 
  mutate(
    vowel = droplevels(vowel),
    pc = paste0(participant, '-', collect)   
  ) |> 
  filter(!pc %in% pcs_to_filter) |> 
  ungroup() |> 
  relocate(participant, vowel, F1_50, F2_50) |> 
  lobanov_2()  
```

We visualise the subset.
```{r}
lob2_sub_vowels |> 
  relocate(
    participant, vowel, F1_lob2, F2_lob2
  ) |> 
  plot_vowel_space(
    vowel_colours = vowel_colours,
    means_only=FALSE, 
    facet=FALSE, 
    ellipse=TRUE, 
    point_alpha=0.1
  )
```

We have some extreme values here. This is unsurprising given we are taking 
means across a low number of tokens.

## Single-parameter log-mean normalisation

A nice normalisation method in cases of missing data is the regression approach
outlined by [@barreda2018regression].

We implement it here:
```{r}
# Assume that data has F1 and F2 as separate columns.  (i.e. in the wider form)
logmean_regression <- function(in_data) {
  regression_data <- in_data |> 
    select(participant, vowel, F1_50, F2_50) |> 
    pivot_longer(
      cols = c('F1_50', 'F2_50'), 
      names_to = "formant", 
      values_to="value"
    ) |> 
    mutate(
      vowel_formant = as.factor(paste0(vowel, '_', formant)),
      participant = as.factor(participant),
      log_value = log(value)
    )
  
  regression_model <- lm(
    log_value ~ 0 + participant + vowel_formant, 
    data = regression_data,
    contrasts = list(vowel_formant = contr.sum)
  )
  
  norm_values <- as_tibble(
    dummy.coef(regression_model)$participant, 
    rownames = "participant"
  )
  
  normalised_data <- regression_data |> 
    left_join(norm_values |> rename(speaker_value = value)) |> 
    mutate(
      norm_value = log_value - speaker_value
    ) |> 
    select(participant, norm_value, formant, vowel) |> 
    mutate(
      id = rep(seq(1, (nrow(regression_data)/2)), each=2)
    ) |> 
    pivot_wider(
      names_from = formant,
      values_from = c("norm_value")
    )|> 
    rename(
        F1_logmean = F1_50,
        F2_logmean = F2_50
    )
  
  out_data <- bind_cols(
    in_data, 
    normalised_data |> select(F1_logmean, F2_logmean)
  )
}

# This code makes the 'participant' column distinguish between recording
# sessions (i.e., each recording session is normalised separately).
logmean_vowels <- vowels |> 
  mutate(
    participant_original = participant,
    participant = paste0(participant, '_', collect)
  ) |> 
  logmean_regression()

# Single value log-mean normalisation relies on a single value to scale
# formants for each speaker. This is, roughly speaking, a way of capturing
# vocal tract length difference.
speaker_values <- function(in_data) {
  regression_data <- in_data |> 
    select(participant, vowel, F1_50, F2_50) |> 
    pivot_longer(
      cols = c('F1_50', 'F2_50'), 
      names_to = "formant", 
      values_to="value"
    ) |> 
    mutate(
      vowel_formant = as.factor(paste0(vowel, '_', formant)),
      participant = as.factor(participant),
      log_value = log(value)
    )
  
  regression_model <- lm(
    log_value ~ 0 + participant + vowel_formant, 
    data = regression_data,
    contrasts = list(vowel_formant = contr.sum)
  )
  
  norm_values <- as_tibble(
    dummy.coef(regression_model)$participant, 
    rownames = "participant"
  )
}

speaks <- speaker_values(vowels)
```

We can look at the distribution of 'speaker values'. These are the values which
are used to scale each speakers vowel space.

```{r}
speak_data <- vowels |> 
  group_by(participant) |> 
  summarise(
    n = n(),
    age = first(age)
  ) |> 
  left_join(speaks)

speak_data |> 
  ggplot(
    aes(
      x = n,
      y = value,
      colour = age
    )
  ) +
  geom_point()
```

Stability seems to increase with data here. This leads to a worry that we don't
even have enough data for this low-data approach! However, given there is only
a handful of 200+ token speakers, it might just be chance that all
have values around $7.1$.

```{r}
logmean_vowels |> 
  relocate(
    participant, vowel, F1_logmean, F2_logmean
  ) |> 
  plot_vowel_space(
    vowel_colours = vowel_colours,
    means_only = FALSE,
    ellipses = TRUE,
    facet = FALSE
  )
```

A bigger concern: we know that there are features of 
children and adult production that are likely to affect one formant more than 
the other. Single value normalisation is designed to avoid normalising such
features! So we explore Lobanov 2.0 with
imputed values and two-parameter log-mean normalisation.

## Two-parameter logmean

We do the same as the above, but treat F1 and F2 individually.

```{r}
# Assume that data has F1 and F2 as separate columns.  (i.e. in the wider form)
dual_logmean_regression <- function(in_data) {
  regression_data <- in_data |> 
    select(participant, vowel, F1_50, F2_50) |> 
    pivot_longer(
      cols = c('F1_50', 'F2_50'), 
      names_to = "formant", 
      values_to="value"
    ) |> 
    mutate(
      participant = as.factor(participant),
      log_value = log(value)
    )
  
  speaker_values <- regression_data |> 
    ungroup() |> 
    group_by(formant) |> 
    nest() |> 
    mutate(
      lm_fit = map(
        data, 
        ~ lm(
          log_value ~ 0 + participant + vowel,
          data = .x,
          contrasts = list(vowel = contr.sum)
          )
      ),
      norm_values = map(
        lm_fit,
        ~ as_tibble(
          dummy.coef(.x)$participant,
          rownames = "participant"
        )
      )
    ) |> 
    select(formant, norm_values) |> 
    unnest(norm_values)
  
  normalised_data <- regression_data |> 
    left_join(speaker_values |> rename(speaker_value = value)) |> 
    mutate(
      norm_value = log_value - speaker_value
    ) |> 
    select(participant, norm_value, formant, vowel) |> 
    mutate(
      id = rep(seq(1, (nrow(regression_data)/2)), each=2)
    ) |> 
    pivot_wider(
      names_from = formant,
      values_from = c("norm_value")
    )|> 
    rename(
        F1_logmean = F1_50,
        F2_logmean = F2_50
    )
  
  out_data <- bind_cols(
    in_data, 
    normalised_data |> select(F1_logmean, F2_logmean)
  )
}

dual_logmean_vowels <- vowels |> 
  mutate(
    participant_original = participant,
    participant = paste0(participant, '_', collect)
  ) |> 
  dual_logmean_regression()
```

Let's look:
```{r}
dual_logmean_vowels |> 
  relocate(
    participant, vowel, F1_logmean, F2_logmean
  ) |> 
  plot_vowel_space(
    vowel_colours = vowel_colours,
    means_only = FALSE,
    ellipses = TRUE,
    facet = FALSE,
    label_size = 3,
    point_alpha = 0.05
  )
```
This may be of use to us as an alternative to imputation. Our next step is to
modify Lobanov 2.0 by allowing imputation.

## Lobanov with Imputation

We can impute values before we normalise, as in Wilson Black et al. (2023). 
In that paper, imputation occurred within intervals of a speaker's monologue.
So we could impute the mean value _for the speaker as a whole_ for a given vowel type.
We have to think carefully about what the best value to impute for speakers in
the absence of a wider source from the recording. The basic principle is to 
avoid biasing future results and to quantify what we have done and how
many data points are affected by normalisation in order to qualify our later
conclusions.

Do we need a minimum data amount? A sense of the distribution of token counts
for each speaker will be useful here.

```{r}
vowels |>
  count(participant, collect, vowel) |> 
  ggplot(
    aes(
      x = n,
      colour = vowel
    )
  ) +
  geom_freqpoly(stat="density") +
  scale_colour_manual(values = vowel_colours)
```
It's really hard to decide on a limit here.

Consequently, we look the other way. That is, we impute for _everyone_ then
filter depending on how many data points we impute for a given speaker.

To start, we want counts for each participant, collect, vowel combination
*including* missing vowels.

Here we'll remove the "FOOT" vowel to match Brand et al. 2021.
We count up the number of instances of each vowel for each speaker.

```{r}
vowels <- vowels |> 
  filter(!vowel %in% c("HANA", "FOOT")) |> 
  mutate(
    vowel = droplevels(vowel)
  )

pcv_counts <- vowels |> 
  count(participant, collect, vowel, .drop = FALSE)
```

How many 0's?

```{r}
token_counts <- pcv_counts |> 
  count(n)

token_counts
```
This is a lot of situations in which a speaker has no tokens of a given vowel.

What percentage of data points am I proposing to impute here?
```{r}
to_impute = sum(token_counts$nn[1:3])/nrow(vowels) * 100
to_impute
```
An additional `r round(to_impute, 2)`%. However, it is worth noting that these imputed values will
not be present in the resulting dataset. That is, they are used to normalise the
actual readings, rather than to generate new readings.

We have an answer to _when_ to impute: if the number of tokens of a given vowel
for a participant collection point is fewer than 3. 

But _what_ will we impute? Here, one obvious answer is a mean value from
children sufficiently close in age at collection point. What is sufficiently
close? Let's say born within 3 months (i.e. their age in months and 
one months either side) initially, and see if that will work. Experiments on 
this were found to be quite unstable. So, instead, we impute from linear models
for each vowel formant pair.

```{r, eval=T}
vowel_models <- vowels |> 
  ungroup() |> 
  pivot_longer(
    cols = contains('_50'),
    names_to = "formant",
    values_to = "formant_value"
  ) |> 
  group_by(vowel, formant) |> 
  nest() |> 
  mutate(
    lm_fit = map(data, ~ lm(formant_value ~ age + gender, data=.x))
  )
```


What do these models look like? Let's get predictions for each age and gender.

```{r, eval=T}
ages <- seq(min(vowels$age), max(vowels$age))

to_predict <- tibble(
  gender = rep(c("M", "F"), each = length(ages)),
  age = rep(ages, times = 2)
)

# This function just adds the predictions to the tibble of the variables to be
# predicted
get_predictions <- function(model, to_predict) {
  predictions <- to_predict |> 
    mutate(
      fit = predict(model, newdata = to_predict)
    )
}

vowel_models <- vowel_models |> 
  mutate(
    predictions = map(lm_fit, ~ get_predictions(.x, to_predict))
  )

vowel_predictions <- vowel_models |> 
  select(vowel, formant, predictions) |> 
  unnest(predictions)

vowel_predictions
```

Let's visualise (without any indication of error). 
```{r}
vowel_predictions |> 
  filter(
    !formant == "F3_50"
  ) |> 
  ggplot(
    aes(
      x = age,
      y = fit,
      colour = vowel
    )
  ) +
  geom_line() + 
  scale_colour_manual(values = vowel_colours) +
  facet_grid(
    cols = vars(gender),
    rows = vars(formant),
    scales = "free"
  ) +
  labs(
    title = "Imputation models",
    x = "Age (months)",
    y = "Predicted formant frequency"
  )
```

We now have values for imputation. We could begin to peer at the results
of these models now, but we will resist until later stages. There is not 
enough being controlled yet.

The next step is to get the additional rows of data we need for each speaker.
We can then add these rows to their non-imputed data with `bind_rows` and then
apply the `lobanov_2` function from the `nzilbb.vowels` package.

***NB:*** we can't directly compare these to the Lobanov 2.0 top-5-vowels 
results because the vowel inventory is different. If it were possible to compare
the top-5 results with a full complement of vowels we wouldn't have had to go 
through this whole imputation process.

It will be convenient to widen the predictions.
```{r}
vowel_predictions <- vowel_predictions |> 
  pivot_wider(
    names_from = 'formant',
    values_from = 'fit'
  )
```

```{r}
get_additional_rows <- function(
    speaker_data, 
    vowel_predictions,
    cutoff = 3
  ) {
  
  # assume speaker_data is for a single collection point, so there is one
  # age and gender.
  age <- speaker_data$age[[1]]
  gender <- speaker_data$gender[[1]]
  
  filtered_predictions <- vowel_predictions |> 
    filter(
      age == age,
      gender == gender
    )
  
  # Find the low-n rows
  to_impute <- speaker_data |> 
  count(vowel, .drop=FALSE) |> 
  filter(
    n < cutoff
  )
  
  # generate new rows with imputed mean values to get up to 3 total for
  # each vowel type.
  out_data <- to_impute |> 
    mutate(
      additional_rows = map2(
        vowel,
        (3-n),
        ~ tibble(
          vowel = rep(.x, .y),
          F1_50 = rep(
            filtered_predictions |> 
              filter(vowel == .x) |> 
              pull(F1_50) |> 
              pluck(1),
            .y
          ),
          F2_50 = rep(
            filtered_predictions |> 
              filter(vowel == .x) |> 
              pull(F2_50) |> 
              pluck(1),
            .y
          )
        )
      )
    ) |> 
    select(additional_rows) |> 
    unnest(additional_rows) |> 
    # Use MatchId column to track which data points have been imputed
    mutate(
      MatchId = "Imputed"
    )
}
```

Let's test the function on a particular speaker at a particular collection
point.

```{r}
test_speaker_data <- vowels |> 
  filter(
    # a random participant
    participant == vowels$participant[[
      floor(runif(1, min = 1, max = nrow(vowels)))
    ]]
  )

# ensure a single collect
test_speaker_data <- test_speaker_data |> 
  filter(
    collect == first(collect)
  )

age <- test_speaker_data$age[[1]]
gender <- test_speaker_data$gender[[1]]

additional_rows <- get_additional_rows(
  test_speaker_data,
  vowel_predictions
)

test_speaker_data |> 
  relocate(
    participant, vowel, F1_50, F2_50
  ) |> 
  plot_vowel_space(
    means_only = FALSE,
    point_alpha = 1
  )

additional_rows
```

These results are plausible. Rerunning the cell lets you look at multiple
participant-collects and their vowel data. If there is no point in the plot,
then there are three in the additional rows, etc.

We can now generate Lobanov 2.0 values for each speaker.
```{r}
lob2_imputed_values <- vowels |> 
  ungroup() |> 
  group_by(participant, collect) |> 
  nest() |> 
  mutate(
    additional_rows = map(
      data,
      ~ get_additional_rows(
        .x,
        vowel_predictions
      )
    ),
    imputed_data = map2(
      data,
      additional_rows,
      ~ .x |> 
        select(vowel, F1_50, F2_50, MatchId) |> 
        bind_rows(.y)
    )
  ) |> 
  ungroup() |> 
  select(
    participant, collect, imputed_data
  ) |>
  unnest(imputed_data) |> 
  mutate(
    pc = paste0(participant, '-', collect)
  ) |> 
  relocate(
    pc, vowel, F1_50, F2_50
  ) |> 
  lobanov_2()
```

We now join these with the original data by `MatchId` to remove the imputed
data points. But first, we collect relevant counts of imputed values for 
each `pc`.

```{r}
# Want to know if any pcs have _no_ imputation (unlikely)
pcs_with_data <- vowels |> 
  select(participant, collect) |> 
  mutate(
    pc = str_c(participant, collect, sep = '-')
  ) |> 
  pull(pc) |> 
  unique()

imputation_counts <- lob2_imputed_values |> 
  mutate(
    participant = as.factor(participant),
    collect = as.factor(collect)
  ) |> 
  filter(
    MatchId == "Imputed"
  ) |> 
  group_by(participant, collect, .drop = FALSE) |> 
  summarise(
    n_vowels = n_distinct(vowel),
    n_tokens = n()
  ) |> 
  mutate(
    pc = str_c(participant, collect, sep = '-')
  ) |> 
  filter(
    pc %in% pcs_with_data
  )

imputation_counts
```

How many imputed tokens?
```{r}
imputation_counts |> 
  ggplot(
    aes(
      x = n_tokens
    )
  ) +
  geom_histogram(bins = 30) +
  labs(
    title = "Distributions of counts of tokens imputed",
    x = "Tokens imputed",
    y = "Collection points"
  )
```

How many imputed vowel types?
```{r}
imputation_counts |> 
  ggplot(
    aes(
      x = n_vowels
    )
  ) +
  geom_histogram(bins = 11) +
  labs(
    title = "Distributions of counts of vowel types imputed",
    x = "Vowel types imputed",
    y = "Collection points"
  ) +
  scale_x_continuous(breaks=seq(1,10))
```

This is a lot of imputation. In the absence of a clear cut off point, we remove
any collection points in which there are more than 20 imputed tokens.

```{r}
to_remove <- imputation_counts |> 
  filter(
    n_tokens > 20
  ) |> 
  mutate(
    pc = str_c(participant, collect)
  )

lob2_imputed <- vowels |> 
  left_join(
    lob2_imputed_values |> 
      select(
        MatchId, F1_lob2, F2_lob2
      )
  ) |> 
  mutate(
    pc = str_c(participant, collect)
  ) |> 
  filter(
    !(pc %in% to_remove$pc)
  ) |> 
  select(-pc)
```

Let's have a look at the resulting data set.
```{r}
lob2_imputed |> 
  relocate(
    participant, vowel, F1_lob2, F2_lob2
  ) |> 
  plot_vowel_space(
    vowel_colours = vowel_colours,
    means_only = FALSE,
    ellipses = TRUE,
    facet = FALSE,
    point_alpha = 0.05
  )
```

# Merge and Export Data

We merge the data together so that all normalisation methods are within the
same data frame.

```{r}
processed_vowels <- vowels |> 
  left_join(
    lob2_imputed |> 
      select(MatchId, F1_lob2, F2_lob2)
  ) |> 
  left_join(
    lob2_sub_vowels |> 
      select(MatchId, F1_lob2, F2_lob2) |> 
      rename(
        F1_sub = F1_lob2,
        F2_sub = F2_lob2
      )
  ) |> 
  left_join(
    logmean_vowels |> 
      select(MatchId, F1_logmean, F2_logmean) |> 
      rename(
        F1_logmean1 = F1_logmean,
        F2_logmean1 = F2_logmean
      )
  ) |> 
  left_join(
    dual_logmean_vowels |> 
      select(MatchId, F1_logmean, F2_logmean) |> 
      rename(
        F1_logmean2 = F1_logmean,
        F2_logmean2 = F2_logmean
      )
  )
```

One last check of how closely correlated these normalised values are:

```{r}
norm_values <- processed_vowels |> 
  select(-F1, -F2) |> 
  select(
    MatchId, matches('F[1-2]'), -matches('band'), -matches('50'), 
    -matches('outlier')
  ) |> 
  pivot_longer(
    cols = -matches('MatchId'),
    names_to = "variable",
    values_to = "norm_value"
  ) |> 
  mutate(
    formant = str_sub(variable, 1, 2),
    norm = str_sub(variable, 4)
  ) |> 
  select(-variable) |> 
  pivot_wider(
    names_from = norm,
    values_from = norm_value
  )
```

First, we look at correlations of the normalisation methods which apply to all
of the data.

```{r}
norm_values |> 
  select(
    -contains('sub'), -MatchId, -formant
  ) |> 
  cor(use="pairwise.complete.obs") |> 
  ggcorrplot(method = "square", lab = TRUE)
```

Now for the top five vowel types:

```{r}
norm_values |> 
  select(
    -MatchId, -formant
  ) |> 
  cor(use='complete.obs') |> 
  ggcorrplot(method = "square", lab=TRUE)
```

Finally, we export the data for modelling.

```{r}
write_rds(processed_vowels, here('data', 'processed_vowels.rds'))
```

# References {-}

```{r}
#| echo: false
grateful::nocite_references(
  grateful::cite_packages(
    output = "citekeys", 
    out.dir = here(), 
    errors="ignored"
  )
)
```

::: refs

:::
